{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bRvbOK2Vu6lP"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import csv\n",
        "import argparse\n",
        "from functools import partial\n",
        "import itertools\n",
        "import uuid\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "from torchtext.vocab import Vocab, build_vocab_from_iterator\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import time\n",
        "import importlib\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a8ppInEHu853"
      },
      "outputs": [],
      "source": [
        "# !pip install selenium\n",
        "# !pip install webdriver-manager\n",
        "# !pip install pickle5\n",
        "# !pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ_CiboAvAvm",
        "outputId": "74946f33-9706-4459-a04a-9df85754a955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Using cuda.\n"
          ]
        }
      ],
      "source": [
        "COLAB = True\n",
        "\n",
        "USE_CUDA = False\n",
        "if COLAB:\n",
        "    from google.colab import drive \n",
        "    drive.mount('/content/gdrive')\n",
        "    PATH = 'gdrive/MyDrive/nlp22/project/'\n",
        "    sys.path.append('gdrive/MyDrive/nlp22/project')\n",
        "\n",
        "    USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "    if USE_CUDA:\n",
        "        DEVICE = torch.device('cuda')\n",
        "        print(\"Using cuda.\")\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "        print(\"Using cpu.\")\n",
        "\n",
        "    os.chdir(os.path.join(os.getcwd(),'gdrive/MyDrive/nlp22/project'))\n",
        "\n",
        "from album_loader import *\n",
        "import lyric_loader\n",
        "import nlpmodel\n",
        "importlib.reload(nlpmodel)\n",
        "\n",
        "# VECTORS_CACHE_DIR = './.vector_cache'\n",
        "\n",
        "UNK, PAD, LBS, LBE, SBS, SBE, PART = 0, 1, 2, 3, 4, 5, 6\n",
        "FIRST_TOKENS = 5000\n",
        "STRATEGY = f'FIRST {FIRST_TOKENS} - Embeddings On'\n",
        "EMBEDDING_DIMENSIONS = 300\n",
        "\n",
        "RATE_TYPE = 'c_rate'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wtVw3Y2vO5G",
        "outputId": "3ff14b5e-e152-4a47-bbd8-ac776aaa1a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW9G_2civLoA",
        "outputId": "7d89dc46-f7a7-415f-ff8e-1cd042c4472f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.21.1\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6VX2NWSQ65-_"
      },
      "outputs": [],
      "source": [
        "def init_albums(path, file, standardize_parts, see_lbs):\n",
        "    albums_data = os.path.join(path, file)\n",
        "    albums_pre = lyric_loader.RegAlbums(album_path = albums_data, \n",
        "                                        standardize_parts = standardize_parts, \n",
        "                                        see_line_breaks = see_lbs)\n",
        "    reg_albums = albums_pre.reg_full_album_text() \n",
        "    return reg_albums\n",
        "\n",
        "def getData(data_list, rating_type):\n",
        "    x = [i[3] for i in data_list]\n",
        "    if rating_type == 'c_rate':\n",
        "        y = [int(i[1]) for i in data_list]\n",
        "    else:\n",
        "        y = [int(10 * i[2]) for i in data_list]\n",
        "    return x, y\n",
        "  \n",
        "def split_datasets(reg_albums, rating_type):\n",
        "    num_train_valid = int(len(reg_albums) * 0.8)\n",
        "    num_test = len(reg_albums) - num_train_valid\n",
        "    train_valid_data, test_data = random_split(reg_albums, [num_train_valid, num_test])\n",
        "\n",
        "    num_train = int(num_train_valid * 0.90)\n",
        "    num_valid = num_train_valid - num_train\n",
        "    train_data, valid_data = random_split(train_valid_data, [num_train, num_valid])\n",
        "\n",
        "    x_train, y_train = getData(train_data, 'c_rate')\n",
        "    x_valid, y_valid = getData(valid_data, 'c_rate')\n",
        "    x_test, y_test = getData(test_data, 'c_rate')\n",
        "    \n",
        "    return x_train, y_train, x_valid, y_valid, x_test, y_test\n",
        "\n",
        "def create_encodings(tokenizer, text, **kwargs):\n",
        "    max_length = None\n",
        "    if 'max_length' in kwargs:\n",
        "        max_length = kwargs['max_length']\n",
        "    \n",
        "    if 'add_special_tokens' in kwargs:\n",
        "        add_special_tokens = kwargs['add_special_tokens']\n",
        "    \n",
        "    encodings = tokenizer(text, truncation=True, padding=True, \n",
        "                                max_length=max_length, add_special_tokens = add_special_tokens)\n",
        "    return encodings\n",
        "\n",
        "class MakeTorchData(torch.utils.data.Dataset):\n",
        "      def __init__(self, encodings, labels):\n",
        "          self.encodings = encodings\n",
        "          self.labels = labels\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          item = {}\n",
        "          for k, v in self.encodings.items():\n",
        "              if torch.is_tensor(v):\n",
        "                  item[k] = v[idx]\n",
        "              else:\n",
        "                item[k] = torch.tensor(v[idx])\n",
        "          item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "          item[\"labels\"] = float(item[\"labels\"])\n",
        "          return item\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iMmXvZ2mOci9"
      },
      "outputs": [],
      "source": [
        "def chunker(item, chunksize):\n",
        "    newObs = []\n",
        "    input_id_chunks = list(item['input_ids'].split(chunksize - 2))\n",
        "    mask_chunks = list(item['attention_mask'].split(chunksize - 2))\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        if input_id_chunks[i][-1].item() == 0:\n",
        "            break\n",
        "        input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "        # add attention tokens to attention mask\n",
        "        mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "        # get required padding length\n",
        "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "        # check if tensor length satisfies required chunk size\n",
        "        if pad_len > 0:\n",
        "            # if padding length is more than 0, we must add padding\n",
        "            input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "            mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "        newDictItem = {}\n",
        "        newDictItem['input_ids'] = input_id_chunks[i]\n",
        "        newDictItem['attention_mask'] = mask_chunks[i]\n",
        "        newDictItem['labels'] = item['labels']\n",
        "        newObs.append(newDictItem)\n",
        "    return newObs\n",
        "\n",
        "def collate_batch_into_chunks(features):\n",
        "    newObs = []\n",
        "    for item in features:\n",
        "        newObs.extend(chunker(item, 512))\n",
        "    return transformers.default_data_collator(newObs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "teX2KyQLGrCA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def compute_metrics_for_regression(eval_pred):\n",
        "    print(\"I am computing the metrics for regression...\")\n",
        "    print(f\"Here is the type of eval_pred input to this function {type(eval_pred)}\")\n",
        "    print(f\"Now, here is the actual value of eval_pred {eval_pred}\")\n",
        "    logits, labels = eval_pred\n",
        "    print(f\"Here is the length of logits: {len(logits)}\")\n",
        "    \n",
        "    labels = labels.reshape(-1, 1)\n",
        "\n",
        "    print(\"Logits:\", logits[0:5])\n",
        "    print(\"Labels:\", labels[0:5])\n",
        "    \n",
        "    mse = mean_squared_error(labels, logits)\n",
        "    var = np.var(labels)\n",
        "    r2 = r2_score(labels, logits)\n",
        "    \n",
        "    single_squared_errors = ((logits - labels).flatten()**2).tolist()\n",
        "    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)\n",
        "\n",
        "        \n",
        "    return {\"mse\": mse, \"var\": var, \"r2\": r2, \"accuracy\" : accuracy}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_train_test(standardize_parts, see_lbs):\n",
        "    print(\"Create train test was called...\")\n",
        "    sp = 1 if standardize_parts else 0\n",
        "    sl = 1 if see_lbs else 0\n",
        "    u_rate_min = 10\n",
        "    reg_albums = init_albums(path = '', file = 'albums_f.pickle', \n",
        "                standardize_parts = standardize_parts, see_lbs = see_lbs, u_rate_min = u_rate_min)\n",
        "\n",
        "    fourth = len(reg_albums) // 4\n",
        "    train_val, test = random_split(reg_albums, [len(reg_albums) - fourth, fourth])\n",
        "\n",
        "    comb = (train_val, test)\n",
        "    with open(f'train_val_test_{sp}_{sl}.pickle', 'wb') as handle:\n",
        "        pickle.dump(comb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return comb\n",
        "\n",
        "def load_train_test(standardize_parts, see_lbs):\n",
        "    sp = 1 if standardize_parts else 0\n",
        "    sl = 1 if see_lbs else 0\n",
        "    try:\n",
        "        with open(f'train_val_test_{sp}_{sl}.pickle', 'rb') as handle:\n",
        "            comb = pickle.load(handle)   \n",
        "    except:\n",
        "        print(f\"Creating train/val/test sets for standardize_parts: {standardize_parts}, see_lbs: {see_lbs}\")\n",
        "        comb = create_train_test(standardize_parts, see_lbs)\n",
        "    train, test = comb\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tsCs70uEnZVb"
      },
      "outputs": [],
      "source": [
        "def main(methodology, \n",
        "          methodologies, \n",
        "          tokenizer,\n",
        "          save_label,\n",
        "          max_length,\n",
        "          evaluation_strategy = 'epoch',\n",
        "          save_strategy = 'epoch',\n",
        "          save_total_limit = 1,\n",
        "          learning_rate = 5e-5,\n",
        "          per_device_train_batch_size = 16,\n",
        "          per_device_eval_batch_size = 16,\n",
        "          num_train_epochs = 20,\n",
        "          weight_decay = 0,\n",
        "          load_best_model_at_end = True,\n",
        "          metric_for_best_model = 'r2',\n",
        "          compute_metrics_for_regression = compute_metrics_for_regression,\n",
        "          collate_batch_into_chunks = collate_batch_into_chunks\n",
        "         ):\n",
        "    \n",
        "    id = uuid.uuid4()\n",
        "\n",
        "    methodology_name = methodologies[methodology][0]\n",
        "    model_name = methodologies[methodology][1]\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.add_tokens(['<lb>', '</lb>', '<sb>', '</sb>', '[part]'])\n",
        "\n",
        "    create_encodings_fx = methodologies[methodology][2]\n",
        "    make_datasets_fx = methodologies[methodology][3]\n",
        "    standardize_parts = methodologies[methodology][4]\n",
        "    see_lbs = methodologies[methodology][5]\n",
        "    chunk = methodologies[methodology][6]\n",
        "\n",
        "    if methodology == 0:\n",
        "        max_length = 512\n",
        "    if chunk:\n",
        "        print(\"Creating chunks so...\")\n",
        "        max_length = 40000\n",
        "        per_device_eval_batch_size = 1\n",
        "    else:\n",
        "        collate_batch_into_chunks = None\n",
        "    \n",
        "    print(f\"Max length is now {max_length}\")\n",
        "    print(f\"Collate function is now {collate_batch_into_chunks}\")\n",
        "    \n",
        "    print(f\"Running following model: {methodology_name}\")\n",
        "    print(f\"Methodology: {methodology_name}\")\n",
        "    print(f\"Create encodings function: {create_encodings_fx}\")\n",
        "    print(f\"Standardize parts: {standardize_parts}\")\n",
        "    print(f\"See line breaks: {see_lbs}\")\n",
        "    print(f\"Chunking: {chunk}\")\n",
        "\n",
        "    reg_albums, test_reg_albums = load_train_test(standardize_parts, see_lbs)\n",
        "    print(f\"Working with {len(reg_albums)} albums in total for reg_albums...\")\n",
        "    \n",
        "\n",
        "    x_train, y_train, x_valid, y_valid, x_test, y_test = split_datasets(reg_albums, 'c_rate')\n",
        "    \n",
        "\n",
        "    print(\"Creating train encodings...\")\n",
        "    train_encodings = create_encodings_fx(tokenizer, x_train, max_length = max_length, add_special_tokens = False)\n",
        "    print(\"Creating valid encodings...\")\n",
        "    valid_encodings = create_encodings_fx(tokenizer, x_valid, max_length = max_length, add_special_tokens = False)\n",
        "    print(\"Creating test encodings...\")\n",
        "    test_encodings = create_encodings_fx(tokenizer, x_test, max_length = max_length, add_special_tokens = False)\n",
        "    \n",
        "    train_dataset = make_datasets_fx(train_encodings, y_train)\n",
        "    valid_dataset = make_datasets_fx(valid_encodings, y_valid)\n",
        "    test_dataset =  make_datasets_fx(test_encodings, y_test)\n",
        "    \n",
        "    print(train_dataset[0])\n",
        "    print(len(train_dataset[0]['input_ids']))\n",
        "    f\"{save_label}/test_dataset\"\n",
        "    print(f\"After creating the datasets, the length of the training set is: {len(train_dataset)}\")\n",
        "    print(f\"The length of the validation set is: {len(valid_dataset)}\")\n",
        "    print(f\"The length of the test set is: {len(test_dataset)}\")\n",
        "        \n",
        "    print(\"Finalized dataset creation, moving on to model...\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 1, ignore_mismatched_sizes = True) # np.log(1000)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    \n",
        "    if USE_CUDA:\n",
        "        model = model.cuda()\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        f\"./{save_label}\",\n",
        "        evaluation_strategy = evaluation_strategy,\n",
        "        save_strategy = save_strategy,\n",
        "        save_total_limit = save_total_limit,\n",
        "        learning_rate = learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        load_best_model_at_end=load_best_model_at_end,\n",
        "        metric_for_best_model=metric_for_best_model\n",
        "    )\n",
        "    info = [model_name, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, num_train_epochs]\n",
        "    \n",
        "    print(collate_batch_into_chunks)\n",
        "    print(\"Instantiating the Trainer...\")\n",
        "    # # Call the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,                         # the instantiated Transformers model to be trained\n",
        "        args=args,                  # training arguments, defined above\n",
        "        train_dataset=train_dataset,         # training dataset\n",
        "        eval_dataset=valid_dataset,          # evaluation dataset\n",
        "        data_collator = collate_batch_into_chunks,\n",
        "        compute_metrics=compute_metrics_for_regression    # the callback that computes metrics of interest\n",
        "    )\n",
        "\n",
        "    print(\"Training the model...\")\n",
        "    # # # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Evaluating the model using evaluation dataset...\")\n",
        "    # # Call the summary\n",
        "    a = trainer.evaluate()\n",
        "    print(\"Returned from evaluate on evaluation set...\")\n",
        "    print(type(a))\n",
        "    print(a)\n",
        "\n",
        "    print(\"Evaluating the model using test dataset...\")\n",
        "    b = trainer.evaluate(test_dataset)\n",
        "    print(\"Returned from evaluate on test set...\")\n",
        "    print(type(b))\n",
        "    print(bin)\n",
        "    return a, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5WTdHsSF_uV9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TNNNVC0s5s_a",
        "outputId": "0a35c86f-4d23-44ad-b810-7f6d4e1e8c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length is now 512\n",
            "Collate function is now None\n",
            "Running following model: BERT First 512\n",
            "Methodology: BERT First 512\n",
            "Create encodings function: <function create_encodings at 0x7ff743716a70>\n",
            "Standardize parts: True\n",
            "See line breaks: False\n",
            "Chunking: False\n",
            "After making limitations, working with 2856 albums in total...\n",
            "2814/2856 (98.5%) albums, have length >200 and are retained.\n",
            "Creating train encodings...\n",
            "Creating valid encodings...\n",
            "Creating test encodings...\n",
            "{'input_ids': tensor([30524,  4873,  2058,  1996, 10098,  1010,  2126,  2039,  2152,  2045,\n",
            "         1005,  1055,  1037,  2455,  2008,  1045,  2657,  1997,  2320,  1999,\n",
            "         1037, 29149,  4873,  2058,  2008, 10098,  1010, 15717,  2024,  2630,\n",
            "         1998,  1996,  5544,  2008,  2017,  8108,  2000,  3959,  2428,  2079,\n",
            "         2272,  2995, 13834,  1045,  1005,  2222,  4299,  2588,  1037,  2732,\n",
            "         1998,  5256,  2039,  2073,  1996,  8044,  2024,  2521,  2369,  2033,\n",
            "         2073, 13460, 14899,  2066, 14380,  9010,  2152,  2682,  1996, 17321,\n",
            "        13284,  1010,  2008,  1005,  1055,  2073,  2017,  1005,  2222,  2424,\n",
            "         2033,  4873,  2058,  1996, 10098,  1010,  2630, 12887,  4875,  5055,\n",
            "         4875,  2058,  2008, 10098,  2339,  1010,  2821,  2339,  1010,  2064,\n",
            "         1005,  1056,  1045,  1029,  2065,  3407,  2210,  2630, 12887,  4875,\n",
            "         3458,  1996, 10098,  2339,  1010,  2821,  2339,  1010,  2064,  1005,\n",
            "         1056,  1045,  1029, 30525, 30524,  2043,  2017,  1005,  2128,  5629,\n",
            "         2043,  2017,  1005,  2128,  5629,  1996,  2878,  2088,  8451,  2007,\n",
            "         2017,  2043,  2017,  1005,  2128,  5870,  2043,  2017,  1005,  2128,\n",
            "         5870,  1996,  3103,  3310,  9716,  2083,  2021,  2043,  2017,  1005,\n",
            "         2128,  6933,  2017,  3288,  2006,  1996,  4542,  2061,  2644,  2008,\n",
            "        19381,  2022,  3407,  2153,  2043,  2017,  1005,  2128,  5629,  2562,\n",
            "         2006,  5629,  1996,  2878,  2088,  8451,  2007,  2017,  2065,  2017,\n",
            "         3402,  2424,  2041,  2017,  1005,  2310,  2042, 11703,  7416,  7178,\n",
            "         2123,  1005,  1056,  2131, 21392,  7178,  2065,  2115,  3129, 14969,\n",
            "         2135,  4136,  2017,  2017,  1005,  2128,  2205, 18890,  2123,  1005,\n",
            "         1056,  2017, 13433,  4904,  1998,  2005,  6014,  1005,  1055,  8739,\n",
            "         2015,  9279,  1037,  5475, 21745,  2065,  1037,  8872,  7365,  2039,\n",
            "         1998,  2398,  2017,  1037,  4942,  6873,  8189,  2065,  1996, 18087,\n",
            "         2323,  2202,  1037,  9898,  2096,  2017,  1005,  2128, 10998,  2091,\n",
            "         1996, 12485,  2123,  1005,  1056, 27874,  1998, 13673,  2138,  2002,\n",
            "         1005,  1055, 10676,  2074,  2227,  1996,  2088,  1998,  2868,  1005,\n",
            "         3426,  2043,  2017,  1005,  2128,  5390,  2378,  1005,  2123,  1005,\n",
            "         1056,  2017,  2113,  2008,  2115,  2191,  1011,  2039,  4627,  2000,\n",
            "         2448,  1998,  2115,  2159,  2131,  2417,  1998, 15121,  7685,  5293,\n",
            "         2115, 13460,  2031,  4426,  1037,  2210,  4569,  2031,  1037,  3608,\n",
            "         5293,  2009,  2035,  5293,  2115, 13460,  2272,  2006,  2131,  3407,\n",
            "         2043,  2017,  1005,  2128,  5629,  1005,  3426,  2043,  2017,  1005,\n",
            "         2128,  5629,  1996,  2878,  2088,  8451,  2007,  2017, 30525, 30524,\n",
            "         2054,  1037,  2154,  2023,  2038,  2042,  2054,  1037,  4189,  6888,\n",
            "         1045,  1005,  1049,  1999,  2339,  2009,  1005,  1055,  2471,  2066,\n",
            "        21388,  2078,  1005,  1999,  2293,  2045,  1005,  1055,  1037,  2868,\n",
            "         2006,  2026,  2227,  2005,  1996,  2878,  2529,  2679,  2339,  2009,\n",
            "         1005,  1055,  2471,  2066, 21388,  2078,  1005,  1999,  2293,  2035,\n",
            "         1996,  2189,  1997,  2166,  3849,  2000,  2022,  2066,  1037,  4330,\n",
            "         2008,  2003,  3614,  2378,  1005,  2005,  2033,  1998,  2013,  1996,\n",
            "         2126,  2008,  1045,  2514,  2043,  2008,  4330,  4627,  2000, 14113,\n",
            "         1045,  2071,  2425,  1045,  2001,  4634,  1045,  2052,  8415,  1045,\n",
            "         2001,  4634,  2009,  1005,  1055,  2471,  2066,  2108,  1999,  2293,\n",
            "         2471,  2066,  2108,  1999,  2293,  2009,  1005,  1055,  2471,  2009,\n",
            "         1005,  1055,  2471,  2021,  2023,  2064,  1005,  1056,  2022,  2293,\n",
            "         2138,  1045,  2514,  2061,  2092,  2053, 21503,  1010,  2053, 14038,\n",
            "         2015,  1010,  2053, 19906,  2023,  2064,  1005,  1056,  2022,  2293,\n",
            "         1010,  1045,  2131,  2053, 14849, 11750,  2026,  2132,  2003,  2025,\n",
            "         1999,  1996,  3712,  2026,  2540,  2515,  2025,  3233,  2145,  2963,\n",
            "         2009,  3786]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': 69.0}\n",
            "512\n",
            "After creating the datasets, the length of the training set is: 2055\n",
            "The length of the validation set is: 229\n",
            "The length of the test set is: 572\n",
            "Finalized dataset creation, moving on to model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([1, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 2055\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1290\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n",
            "Instantiating the Trainer...\n",
            "Training the model...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1290' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1290/1290 17:20, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Mse</th>\n",
              "      <th>Var</th>\n",
              "      <th>R2</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2857.026123</td>\n",
              "      <td>2857.025635</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-32.615396</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>895.021301</td>\n",
              "      <td>895.021301</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-9.530704</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>86.924179</td>\n",
              "      <td>86.924179</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-0.022739</td>\n",
              "      <td>0.034934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1705.524600</td>\n",
              "      <td>87.243263</td>\n",
              "      <td>87.243263</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-0.026493</td>\n",
              "      <td>0.039301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1705.524600</td>\n",
              "      <td>99.849922</td>\n",
              "      <td>99.849922</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-0.174821</td>\n",
              "      <td>0.039301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1705.524600</td>\n",
              "      <td>84.505058</td>\n",
              "      <td>84.505066</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>0.005724</td>\n",
              "      <td>0.061135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1705.524600</td>\n",
              "      <td>73.217690</td>\n",
              "      <td>73.217690</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>0.138530</td>\n",
              "      <td>0.069869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>65.864300</td>\n",
              "      <td>77.713051</td>\n",
              "      <td>77.713066</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>0.085638</td>\n",
              "      <td>0.078603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>65.864300</td>\n",
              "      <td>84.941544</td>\n",
              "      <td>84.941544</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.056769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>65.864300</td>\n",
              "      <td>86.963882</td>\n",
              "      <td>86.963882</td>\n",
              "      <td>84.991592</td>\n",
              "      <td>-0.023206</td>\n",
              "      <td>0.048035</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6ac5cc750>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[21.256742]\n",
            " [21.223179]\n",
            " [21.253834]\n",
            " [21.254168]\n",
            " [21.260906]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-129\n",
            "Configuration saved in ./BERT First 512/checkpoint-129/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-129/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-1161] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6ac8e9c10>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[45.45079 ]\n",
            " [45.42547 ]\n",
            " [45.443764]\n",
            " [45.443398]\n",
            " [45.44888 ]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-258\n",
            "Configuration saved in ./BERT First 512/checkpoint-258/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-258/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-1290] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acb08850>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[72.51901 ]\n",
            " [72.5089  ]\n",
            " [72.5209  ]\n",
            " [72.51377 ]\n",
            " [72.516205]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-387\n",
            "Configuration saved in ./BERT First 512/checkpoint-387/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-387/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-129] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6c4070650>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[75.42979]\n",
            " [75.28631]\n",
            " [75.44517]\n",
            " [75.41584]\n",
            " [75.42215]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-516\n",
            "Configuration saved in ./BERT First 512/checkpoint-516/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-516/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-258] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6ac5cc650>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[78.222435]\n",
            " [78.190125]\n",
            " [78.20078 ]\n",
            " [77.93885 ]\n",
            " [78.206024]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-645\n",
            "Configuration saved in ./BERT First 512/checkpoint-645/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-645/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-516] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acb44190>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[78.97354 ]\n",
            " [78.958046]\n",
            " [78.988106]\n",
            " [74.543045]\n",
            " [78.65009 ]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-774\n",
            "Configuration saved in ./BERT First 512/checkpoint-774/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-774/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-387] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6ac486150>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[79.31385 ]\n",
            " [78.992775]\n",
            " [77.894295]\n",
            " [69.9058  ]\n",
            " [77.29882 ]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-903\n",
            "Configuration saved in ./BERT First 512/checkpoint-903/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-903/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-645] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6ac6cd350>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[80.841286]\n",
            " [81.27626 ]\n",
            " [79.87078 ]\n",
            " [75.67984 ]\n",
            " [77.749405]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-1032\n",
            "Configuration saved in ./BERT First 512/checkpoint-1032/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-1032/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-774] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acad5d90>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[82.16741]\n",
            " [82.14816]\n",
            " [80.70864]\n",
            " [75.42825]\n",
            " [78.80461]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-1161\n",
            "Configuration saved in ./BERT First 512/checkpoint-1161/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-1161/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-1032] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acbc9550>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[82.438194]\n",
            " [82.37202 ]\n",
            " [80.2937  ]\n",
            " [75.64771 ]\n",
            " [78.10065 ]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./BERT First 512/checkpoint-1290\n",
            "Configuration saved in ./BERT First 512/checkpoint-1290/config.json\n",
            "Model weights saved in ./BERT First 512/checkpoint-1290/pytorch_model.bin\n",
            "Deleting older checkpoint [BERT First 512/checkpoint-1161] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./BERT First 512/checkpoint-903 (score: 0.13853022414011906).\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 229\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model using evaluation dataset...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 572\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acb249d0>\n",
            "Here is the length of logits: 229\n",
            "Logits: [[79.31385 ]\n",
            " [78.992775]\n",
            " [77.894295]\n",
            " [69.9058  ]\n",
            " [77.29882 ]]\n",
            "Labels: [[85.]\n",
            " [61.]\n",
            " [85.]\n",
            " [79.]\n",
            " [76.]]\n",
            "Returned from evaluate on evaluation set...\n",
            "<class 'dict'>\n",
            "{'eval_loss': 73.21768951416016, 'eval_mse': 73.21768951416016, 'eval_var': 84.99159240722656, 'eval_r2': 0.13853022414011906, 'eval_accuracy': 0.06986899563318777, 'eval_runtime': 3.7872, 'eval_samples_per_second': 60.466, 'eval_steps_per_second': 3.961, 'epoch': 10.0}\n",
            "Evaluating the model using test dataset...\n",
            "I am computing the metrics for regression...\n",
            "Here is the type of eval_pred input to this function <class 'transformers.trainer_utils.EvalPrediction'>\n",
            "Now, here is the actual value of eval_pred <transformers.trainer_utils.EvalPrediction object at 0x7ff6acb34a90>\n",
            "Here is the length of logits: 572\n",
            "Logits: [[66.87273 ]\n",
            " [78.05265 ]\n",
            " [65.40853 ]\n",
            " [71.547035]\n",
            " [73.48673 ]]\n",
            "Labels: [[79.]\n",
            " [73.]\n",
            " [56.]\n",
            " [80.]\n",
            " [88.]]\n",
            "Returned from evaluate on test set...\n",
            "<class 'dict'>\n",
            "<built-in function bin>\n"
          ]
        }
      ],
      "source": [
        "                      #   Name           /Create encodings /Make dataset/Std Pts/Line breaks/Chunk\n",
        "methodologies = {0 : ('BERT First 512',\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "                      create_encodings, MakeTorchData, True, False, False),\n",
        "                 1 : ('BERT Longformer 4096', \"allenai/longformer-base-4096\",\n",
        "                      create_encodings, MakeTorchData, True, False, False)}\n",
        "train_batch_size = 16\n",
        "test_batch_size = 16\n",
        "save_label = \"BERT First 512\"\n",
        "max_length = 512\n",
        "\n",
        "# Training Arguments\n",
        "evaluation_strategy = 'epoch'\n",
        "save_strategy = 'epoch'\n",
        "save_total_limit = 1\n",
        "learning_rate = 5e-5\n",
        "per_device_train_batch_size = train_batch_size\n",
        "per_device_eval_batch_size = test_batch_size\n",
        "num_train_epochs = 10\n",
        "weight_decay = 0\n",
        "load_best_model_at_end = True\n",
        "metric_for_best_model = 'r2'\n",
        "\n",
        "main_args= {'methodologies' : methodologies,\n",
        "            'tokenizer' : tokenizer,\n",
        "            'save_label': save_label,\n",
        "            'max_length' : max_length,\n",
        "            'evaluation_strategy': evaluation_strategy,\n",
        "            'save_strategy' : save_strategy,\n",
        "            'save_total_limit' : save_total_limit,\n",
        "            'learning_rate' : learning_rate,\n",
        "            'per_device_train_batch_size' : per_device_train_batch_size,\n",
        "            'per_device_eval_batch_size' : per_device_eval_batch_size,\n",
        "            'num_train_epochs' : num_train_epochs,\n",
        "            'weight_decay': weight_decay,\n",
        "            'load_best_model_at_end' : load_best_model_at_end,\n",
        "            'metric_for_best_model' : metric_for_best_model}\n",
        "\n",
        "a, b = main(0, **main_args)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGFtGlVSpkSw",
        "outputId": "5ab7b578-f5bb-4300-fa8e-8d7277bbb766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 73.21768951416016, 'eval_mse': 73.21768951416016, 'eval_var': 84.99159240722656, 'eval_r2': 0.13853022414011906, 'eval_accuracy': 0.06986899563318777, 'eval_runtime': 3.7872, 'eval_samples_per_second': 60.466, 'eval_steps_per_second': 3.961, 'epoch': 10.0}\n",
            "{'eval_loss': 76.2323989868164, 'eval_mse': 76.23239135742188, 'eval_var': 84.88729095458984, 'eval_r2': 0.1019574448215369, 'eval_accuracy': 0.07517482517482517, 'eval_runtime': 9.4004, 'eval_samples_per_second': 60.849, 'eval_steps_per_second': 3.83, 'epoch': 10.0}\n"
          ]
        }
      ],
      "source": [
        "print(a)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpOwpn51OjQc"
      },
      "source": [
        "Graveyard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5iulkWJUK-uR",
        "outputId": "82e4cf4f-4e9c-46a1-f900-b0abe1628e3b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/nlp22/project'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PoDLZDuX7iV"
      },
      "outputs": [],
      "source": [
        "# def chunker(input_ids, attention_mask, curr_label, chunksize):\n",
        "#     input_id_chunks = list(torch.tensor(input_ids).split(chunksize - 2))\n",
        "#     mask_chunks = list(torch.tensor(attention_mask).split(chunksize - 2))\n",
        "#     label_chunks = [curr_label] * len(input_id_chunks)\n",
        "#     for i in range(len(input_id_chunks)):\n",
        "#       # add CLS and SEP tokens to input IDs\n",
        "#       input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "#       # add attention tokens to attention mask\n",
        "#       mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "#       # get required padding length\n",
        "#       pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "#       # check if tensor length satisfies required chunk size\n",
        "#       if pad_len > 0:\n",
        "#           # if padding length is more than 0, we must add padding\n",
        "#           input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "#           mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "#     return input_id_chunks, mask_chunks, label_chunks\n",
        "# # chunker(a, b, 512)\n",
        "\n",
        "# def chunk_encodings(encodings, labels):\n",
        "#     encodings_dict = {'input_ids' : [],\n",
        "#                       'attention_mask' : []}\n",
        "#     y_train_big = []\n",
        "#     for i in range(len(encodings['input_ids'])):\n",
        "#         curr_input_ids = encodings['input_ids'][i]\n",
        "#         curr_mask = encodings['attention_mask'][i] # train_encodings, test_encodings\n",
        "#         curr_label = labels[i] # y_train\n",
        "\n",
        "#         input_id_chunks, mask_chunks, label_chunks = chunker(curr_input_ids, curr_mask, curr_label, 512)\n",
        "\n",
        "#         encodings_dict['input_ids'].extend(input_id_chunks)\n",
        "#         encodings_dict['attention_mask'].extend(mask_chunks)\n",
        "#         y_train_big.extend(label_chunks)\n",
        "#     return encodings_dict, y_train_big"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj9kY04tphBL"
      },
      "source": [
        "**Test Section**`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "d3d23b23216148d785929cd0e324bd1c",
            "18acc9c0e6194cf2b5bb1010ae12ae02",
            "51d38bf4df404b2f86bb33db43fa6bf7",
            "b37851f63a68496e8826f326294f858b",
            "e9514eb50be94889a042e50b460c02e4",
            "35272d617bcf45dfac00420278eea79b",
            "8a3755a65c004fec8b1b89340ece7533",
            "26e480f9a5e94914af20f420e02e6f79",
            "65e1d8e4d0c74ab1ae9f6b409bd7989f",
            "0308fab4dcb448228e9b41ac53f64cb6",
            "bca8269cdc4c433aa419a683a7d960d1",
            "4a8dda7dd2c747c18858fcf3129561c0",
            "c32f27667d824138b0f309f7596ee46c",
            "b54616e138e5498c9a627863a87bd05e",
            "29e928265c8f4c2fa983b4870d731910",
            "d874fc6ba49f46b39fb92818ee5e5e82",
            "2f7b3fc1d1d74520a9437bc41461dde5",
            "693c9e7b5b7f419bb6f23fad55a87e7b",
            "5675bd10fe874e239c974677b5cef207",
            "b58160dfaab9450da3b1a8c1f147f8b1",
            "25cbc225f9e74d73921bde58a48510cc",
            "19c1e6c6552845999c3a9cfde3e3d972",
            "9feaf92695eb4eb793d6262958c9bdac",
            "9ece9fc77a5941f4b0e0f444fbf753d0",
            "e2105eefe33a4646b8774da10082c45c",
            "4ad49e41bf744450b81f3da81b01ee90",
            "44d028338e0d4235a912c6564dac0235",
            "9816859db7434245950a92c72e9f7e56",
            "f5d997cc7dd54d91adb12cf14939643b",
            "d79fb131a3064523b87759729d6af229",
            "0a704bc8e70c4780866ffeba8a325d25",
            "9c3ce84bad264a8bae525e22d11772c6",
            "281db7bc45fc4d4e8e0cfa4f2acf6b05"
          ]
        },
        "id": "T9rNPOGna-Qs",
        "outputId": "caee1b1a-d74c-493d-a5d6-45644bc59298"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3d23b23216148d785929cd0e324bd1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a8dda7dd2c747c18858fcf3129561c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9feaf92695eb4eb793d6262958c9bdac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "methodologies = {0 : ('BERT First 512', create_encodings, MakeTorchData, True, False, False),\n",
        "                 1 : ('BERT Chunk 512', create_encodings, MakeTorchData, True, False, True)}\n",
        "\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_tokens(['<lb>', '</lb>', '<sb>', '</sb>', '[part]'])\n",
        "max_length = 50000\n",
        "batch_size = 16\n",
        "\n",
        "# Training Arguments\n",
        "evaluation_strategy = 'epoch'\n",
        "save_strategy = 'epoch'\n",
        "learning_rate = 5e-5\n",
        "per_device_train_batch_size = batch_size\n",
        "per_device_eval_batch_size = batch_size\n",
        "num_train_epochs = 15\n",
        "weight_decay = 0\n",
        "load_best_model_at_end = True\n",
        "metric_for_best_model = 'r2'\n",
        "\n",
        "main_args= {'methodologies' : methodologies,\n",
        "            'model_name' : model_name,\n",
        "            'tokenizer' : tokenizer,\n",
        "            'evaluation_strategy': evaluation_strategy,\n",
        "            'save_strategy' : save_strategy,\n",
        "            'learning_rate' : learning_rate,\n",
        "            'per_device_train_batch_size' : per_device_train_batch_size,\n",
        "            'per_device_eval_batch_size' : per_device_eval_batch_size,\n",
        "            'num_train_epochs' : num_train_epochs,\n",
        "            'weight_decay': weight_decay,\n",
        "            'load_best_model_at_end' : load_best_model_at_end,\n",
        "            'metric_for_best_model' : metric_for_best_model}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg4PObro4mbW",
        "outputId": "face5c03-1dc9-47f6-c082-9c8d0c955981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101, 30524,  2210,  4562,  2210,  4562,  2017,  1005,  2128,  2893,\n",
              "           2041,  1997,  2192,  2893,  2041,  1997,  2192,  1045,  2228,  1045,\n",
              "           1005,  1049,  2183,  2000,  4558,  2017,  2085,  2821,  1010,  2210,\n",
              "           4562,  2210,  4562,  1010,  2017,  2113,  2033,  2205,  2092,  4312,\n",
              "           2205,  2092,  2296,  2154,  1045,  1005,  1049,  2183,  2188,  1045,\n",
              "           1005,  1049,  2183,  4218,  1996,  3340,  1045,  1005,  1049,  2183,\n",
              "           2104,  1996,  5800,  2153,  1998,  1045,  2180,  1005,  1056,  2022,\n",
              "           2067,  1999,  1037,  2146,  2051,  1010,  2061,  2131,  2041,  2131,\n",
              "           2041,  1997,  2023,  2214,  2160,  2077,  1045,  6402,  2009,  2091,\n",
              "           1045,  2876,  1005,  1056,  2215,  2000,  3426,  2017,  2505,  2008,\n",
              "           2453,  3338,  2115,  8403,  2227,  1999,  1037,  4595, 10909,  2859,\n",
              "           4109,  1999,  2023, 11655, 19766,  2088,  1997,  3714,  4109, 30525,\n",
              "          30524, 30526,  1045,  2293,  2017,  2083, 12300,  1998,  9716,  8626,\n",
              "           1010,  1045,  2079,  1998,  2085,  2045,  1005,  1055,  4623,  1999,\n",
              "           2019,  4064, 14492,  2064,  1045,  2293,  2017,  2083, 12300,  1998,\n",
              "           9716,  8626,  1010,  1045,  2079,  2085,  2045,  1005,  1055,  9995,\n",
              "           1999,  1037, 11060,  2041, 17184, 30526,  2017,  2288,  2033,  2125,\n",
              "           1996,  3259,  2461,  2074, 15627,  2041,  1997,  1996,  2250,  1996,\n",
              "           2190,  2477,  2272,  2013,  7880,  1045,  2293,  2017,  1010,  1045,\n",
              "           2123,  1005,  1056,  2228,  2017,  2729, 30526,  1045,  2293,  2017,\n",
              "           2083, 12300,  1998,  9716,  8626,  1010,  1045,  2079,  1998,  1996,\n",
              "          14991,  1999,  2115,  2642,  5861,  1045,  2293,  2017,  2083, 12300,\n",
              "           1998,  9716,  8626,  1010,  1045,  2079,  1045,  2064,  2156,  2870,\n",
              "           1999,  1996, 25416, 10484,  2094, 19070,  8026, 30526,  2017,  2288,\n",
              "           2033,  2125,  1996, 10682,  2074, 15627,  2041,  1997,  1996,  2250,\n",
              "           1996,  2190,  2477,  2272,  2013,  7880,  1045,  2064,  1005,  1056,\n",
              "           2903,  2017,  2729,  2729, 30526,  6583,  1010,  6583,  6583,  1010,\n",
              "           6583,  6583,  1010,  6583,  6583,  1010,  6583,  1010,  6583,  4830,\n",
              "           1010,  6583,  6583,  1010,  6583,  6583,  1010,  6583,  6583,  1010,\n",
              "           6583,  6583,  6583,  1045,  2293,  2017,  1010,  1045,  2293,  2017,\n",
              "           1998,  1045,  2342,  2017, 30526,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017,  2748,  1010,  1045,  2903,  2017,\n",
              "           2748,  1010,  1045,  2903,  2017, 30525, 30524,  2009,  1005,  1055,\n",
              "           1015,  1051,  1005,  5119,  2006,  1037,  5958,  2851,  1045,  1005,\n",
              "           1049,  2667,  2000,  2562,  2026,  2067,  2013,  1996,  2813,  1996,\n",
              "          23172,  1998,  2037,  9767,  2031,  2018,  2178,  3112,  1045,  1005,\n",
              "           1049,  6603,  2339,  2057,  8572,  2012,  2035,  1998,  1045,  2228,\n",
              "           1997,  2017,  2006,  3147,  3467, 16956,  1010,  9548,  2027, 10825,\n",
              "           2033,  1997,  2043,  2057,  2020,  1999,  2082,  2498,  2428, 13836,\n",
              "           2043,  2017,  2170,  2041,  2026,  2171,  1999,  2755,  1010,  2498,\n",
              "           2428, 13836,  2012,  2035,  1998,  1045,  2228,  2055,  2129,  2146,\n",
              "           2009,  2097,  2202,  2068,  2000,  6271,  2149,  2185,  2021,  1045,\n",
              "           2180,  1005,  1056,  2131,  2033,  2091,  1045,  1005,  1049,  2074,\n",
              "          18836,  2000,  2022,  5307,  1996,  2154,  3426,  2420,  2123,  1005,\n",
              "           1056,   102]),\n",
              "  'labels': 81.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  2131,  2017,  2521,  2043,  2017,  1005,  2128,  2908,  2009,\n",
              "           1005,  1055,  1019,  1051,  1005,  5119,  2006,  1037,  5958,  2851,\n",
              "           2028,  3634,  7026,  2015,  6073,  1998,  3614,  2028,  1997,  2068,\n",
              "           1005,  1055,  2013,  2619,  2040,  2354,  2017,  2092,  1045,  1005,\n",
              "           2222,  2145,  2228,  1997,  2017,  2006,  3147,  3467, 16956,  1010,\n",
              "           9548,  2027,  1005,  2222,  2145, 10825,  2033,  1997,  2043,  2057,\n",
              "           2020,  2012,  2082,  2043,  2027,  2071,  2196,  2031, 11766,  2033,\n",
              "           2008,  3268,  2066,  6737,  2020,  1999,  1996,  2398,  1997,  2122,\n",
              "           9413, 20793,  3560, 18656,  1998,  2000,  2216,  1997,  2017,  2040,\n",
              "          13673,  2115,  3268,  2083,  2028,  2154,  2000,  1996,  2279,  2092,\n",
              "           1010,  2292,  2068,  2202,  2017,  2000,  2279,  2064,  1005,  1056,\n",
              "           2017,  2444,  1998,  2022, 18836,  2017,  1005,  2128,  2182,  1029,\n",
              "           2156,  2009,  2071,  2022,  2017,  4826,  2279,  2095, 30525, 30524,\n",
              "           2023,  2003,  2073,  2057,  2991,  2013,  1996,  3392,  2023,  2003,\n",
              "           2073,  1996,  3712, 11190,  2545,  2039,  4830,  6199,  2332,  1997,\n",
              "           6569,  2017,  2081,  1037,  2158,  2041,  1997,  2033,  2023,  2003,\n",
              "           2073,  1996,  3221,  3727,  1996, 10014, 27546,  2075,  1037,  6370,\n",
              "           1997,  2814,  1045,  1005,  2222,  8813,  2017,  2467,  2017,  2113,\n",
              "           1045,  2293,  2017,  2017,  2113,  1045,  2293,  2017,  2017,  2113,\n",
              "           1045,  2293,  2017,  2023,  2003,  2073,  2057,  5256,  1999,  1996,\n",
              "          14033,  2023,  2003,  2073,  2256,  4230,  2360,  2053,  2062,  5357,\n",
              "          18108,  2006,  1996,  2723,  3856,  2041,  2256,  2417,  4777,  2061,\n",
              "          10364,  2178, 13803,  2041,  2005,  2033,  2009,  1005,  2222,  2022,\n",
              "           1996,  2197,  5835,  2057,  3745,  2004,  1045, 11852,  2046,  7880,\n",
              "           2113,  2008,  1045,  3866,  2017,  2113,  2008,  1045,  3866,  2017,\n",
              "           2113,  2008,  1045,  3866,  2017,  2021,  2293,  2001,  2025,  2438,\n",
              "           2000,  2907,  2026,  2132,  2064,  1005,  1056,  2017,  2074,  2514,\n",
              "           2026,  3093,  7540,  2046,  2216,  4153,  1998,  3712,  2073,  2057,\n",
              "           2119,  9880,  4153,  1998,  3712,  4214,  2033,  1999,  4153,  1998,\n",
              "           3712,  1010,  1045,  2425,  2870,  1045,  1005,  1049,  2025, 12489,\n",
              "          10334,  2842,  2017,  2113,  1045,  1005,  1049,  2975,  2017,  2113,\n",
              "           1045,  1005,  1049,  2975,  2027,  2113,  1010,  2027,  2113,  2027,\n",
              "           2113,  1010,  2113,  2975,  1010,  2975,  1010,  2975,  2113,  2008,\n",
              "           2975,  1010,  2023,  2369,  2975,  2023,  2369,  2821,  1010,  2821,\n",
              "           1010,  2821,  1010, 25391,  2061,  1045,  1005,  1049,  2975,  2026,\n",
              "           2190,  2767,  2074,  2005,  1996,  3109,  1997,  2009,  2074,  2005,\n",
              "           1996,  8739,  1997,  2009,  2021,  2129,  2172,  1045,  3866,  2017,\n",
              "          30525, 30524,  2272,  2185,  2007,  2033,  4009,  6281,  2006,  2026,\n",
              "           3332,  9739,  2063,  2272,  2185,  2007,  2033,  2057,  1005,  2222,\n",
              "           2991,  6680,  1998,  2067,  2153,  2052,  2017,  6807,  2026,  2227,\n",
              "           2065,  2017,  2387,  2033,  1999,  2023,  2173,  2061,  2521,  2185,\n",
              "           2013,  2188,  2272,  1998,  3153,  2007,  2033,  2057,  1005,  2222,\n",
              "          29086,  2057,  1005,  2222, 29086,  1999,  1996, 20981,  4542, 30525,\n",
              "          30524,  8271,  2039,  2007,  1037, 10818,  1999,  2026,  2677,  2323,\n",
              "           1045,  2132,  2167,  2030,  2132,  2148,  1029,  2017,  2288,  2033,\n",
              "           2041,  2006,  1996,  2697,  1998,  1045,  2924,  2035,  2026,  5544,\n",
              "           2006,  2019,  8372,  1998,  1045,  2371,  2293,  2272,  1999,  2083,\n",
              "           2026,  3332,  9739,  2063,  4440,  4691,  2039,  1996,  2152,  2346,\n",
              "          13311,  2091,  1996,  2659,  1998,  1045,  2387,  2166, 22417,  2041,\n",
              "           1996, 25951,  2065,  2017,  2215,  2009,  1010,  2292,  2009,  2175,\n",
              "           2065,  2017,  2215,  2009,  1010,  2061,  2860,  2616,  2064,  1005,\n",
              "           1056,   102]),\n",
              "  'labels': 81.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  4671,  2054,  2009,  2965,  1998,  2748,  1010,  1045,  2903,\n",
              "           2017,  5650,  1998,  1045,  2371,  2293,  2746,  2083,  2026,  3332,\n",
              "           9739,  2063,  4440,  4691,  2039,  1996,  2152,  2346, 13311,  2091,\n",
              "           1996,  2659,  1998,  1045,  2387,  2166, 16883, 11273,  2041,  1996,\n",
              "           3332,  2065,  2017,  2215,  2009,  1010,  2292,  2009,  2175,  2065,\n",
              "           2017,  2215,  2009,  1010,  2360,  2061,  1012,  1012,  1012,  2821,\n",
              "           1010,  2272,  1999,  2083,  2026,  3332,  9739,  2063,  2272,  1999,\n",
              "           2083,  2026,  3332,  9739,  2063,  2180,  1005,  1056,  2017,  2272,\n",
              "           1999,  2083,  2026,  3332,  9739,  2063,  2272,  1999,  2083,  1010,\n",
              "           2272,  1999,  2083,  1010,  2821, 30525, 30524,  1045,  2074,  2055,\n",
              "           3266,  2000,  5293,  2017,  2043,  2017,  3711,  1999,  1037,  3959,\n",
              "           1998,  2017,  1005,  2128,  2130,  2062,  3376,  2045,  2084,  1045,\n",
              "           3342,  2017,  2108,  2061,  1045,  1005,  2310,  2272,  2000,  5630,\n",
              "           2008,  6580,  2003,  4129,  2033,  2017,  2097,  2025,  2175,  1998,\n",
              "           6195,  2023,  1045,  2215,  2017,  2000,  2113,  2065,  1996,  2088,\n",
              "           4515,  1010,  1045,  3246,  2017,  1005,  2128,  2182,  2007,  2033,\n",
              "           1045,  2228,  2057,  2071,  4756,  2074,  2438,  2000,  2025,  3280,\n",
              "           1999,  3255,  2065,  1996,  2088,  4515,  1010,  2009,  2180,  1005,\n",
              "           1056,  3926,  2017,  2017,  1005,  2128,  2025,  1996,  2828,  2027,\n",
              "           2064,  5425,  1010,  2017, 13109,  4183,  2066,  1037,  4875, 13795,\n",
              "           2027,  2064,  1005,  1056,  9231,  2017,  2091,  1010,  2064,  1005,\n",
              "           1056,  9231,  2017,  2091,  1999,  2026,  3959,  2017,  1005,  2128,\n",
              "           2652,  2007, 13610,  2015,  1997,  5472,  1998,  2300,  2001,  2770,\n",
              "           2083,  2119,  1997,  2115,  2398,  1998,  1045,  2123,  1005,  1056,\n",
              "           2228,  1045,  2412,  2657,  2017,  4092,  1005,  3426,  1045,  2001,\n",
              "           2205,  5058,  2039,  1999,  1996,  3959,  1045,  2001, 12802,  2061,\n",
              "           2065,  1996,  2088,  4515,  1010,  1045,  3246,  2017,  1005,  2128,\n",
              "           2011,  2026,  2217,  1045,  2123,  1005,  1056,  2228,  2007,  2017,\n",
              "           2182,  2009,  2097,  2022,  2205,  2172,  3255,  1998,  2043,  2017,\n",
              "           5390,  1010,  9548,  1010,  1045,  4299,  2017,  1005,  1040,  2514,\n",
              "           2026,  2293,  2115,  2540,  2003,  2126,  3458,  5425,  1010, 13109,\n",
              "          12474,  2075,  2066,  1037,  4875, 13795,  2027,  2064,  1005,  1056,\n",
              "           9231,  2017,  2091,  1010, 23281,  1010,  2027,  2064,  1005,  1056,\n",
              "           9231,  2017,  2091,  1010, 23281,  2064,  1005,  1056,  9231,  2017,\n",
              "           2091,  2064,  1005,  1056,  9231,  2017,  2091,  2064,  1005,  1056,\n",
              "           9231,  2017,  2091, 30525, 30524,  1996,  2088,  2003,  2256, 10135,\n",
              "           2085,  1996,  2088,  2003,  2256,  3153,  2723,  2085, 10825,  2033,\n",
              "           2129,  2000,  3153,  2153,  1996,  2088,  2003,  2256, 10135,  2085,\n",
              "           1996,  2088,  2003,  2074,  3403,  2085,  2053,  4582,  2041,  3645,\n",
              "           2085,  2256,  3345,  3030,  3048,  2847,  3283,  2057,  1005,  2128,\n",
              "           2182,  1010,  2057,  1005,  2128,  2182,  1010,  2057,  1005,  2128,\n",
              "           2182,  2748,  1010,  2057,  1005,  2128,  2182,  2489,  2000,  2448,\n",
              "           1998,  5390, 14723,  2000,  3046,  1998,  2498,  2182,  1005,  1055,\n",
              "           4276,  3045,  2302,  1037,  2954,  2043,  1045,  2064,  1005,  1056,\n",
              "           2693,  1045,  1005,  2222,  5959,  1996,  2145,  2005,  1037,  2096,\n",
              "           1998,  4558,  2870,  1999,  5975,  1998,  4020,  1998,  1996,  3712,\n",
              "           1998,  1045,  1005,  1049,  2067,  2182,  4248,  2004,  7407,  1005,\n",
              "           3426,  2057,  2024,  2074,  3823,  3823,  1999,  1037,  2154,  2009,\n",
              "           1005,  1055,  1037,  3153,  2723,  2085,  2053,  4582,  2041,  3645,\n",
              "           2085,  2256,  3345,  3030,  3048,  2847,  3283,  2057,  1005,  2128,\n",
              "           2182,  1010,  2057,  1005,  2128,  2182,  1010,  2057,  1005,  2128,\n",
              "           2182,   102]),\n",
              "  'labels': 81.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  2054,  2079,  1045,  2079,  1029,  2489,  1010,  2489,  2000,\n",
              "           2448,  1998,  5390, 14723,  2000,  3046,  1998,  2498,  2003,  4276,\n",
              "           3045,  2302,  1037,  2954,  2821,  1010,  2748,  1010,  2057,  1005,\n",
              "           2128,  2182,  2489,  2000,  2175,  9577,  6569,  1998,  3255,  2024,\n",
              "           3554,  1999,  1996, 17506,  2503,  2043,  1045,  2064,  1005,  1056,\n",
              "           2693,  1045,  1005,  2222,  7842, 14550,  1996,  8724,  2005,  1037,\n",
              "           2096,  1998,  4558,  2870,  1999,  5975,  1998,  4020,  1998,  1996,\n",
              "           3712,  1998,  1045,  1005,  1049,  2067,  2182,  4248,  2004,  7407,\n",
              "           1005,  3426,  2057,  2024,  2074,  3823,  3823,  1999,  1037,  2154,\n",
              "          30525, 30524, 30526,  2009,  1005,  1055,  2025, 24057,  8870,  2009,\n",
              "           1005,  1055,  2025, 24057,  6077,  1998, 14695,  2024,  2025,  3909,\n",
              "           2030,  3810,  1996,  2522,  5620,  1996,  3103,  2038,  2053,  6045,\n",
              "           2006,  7188,  2009, 12342,  2015,  1998,  1045,  1005,  2310,  2196,\n",
              "           2464,  1037,  4937,  2007,  3157,  3268,  1045,  1005,  1049,  2025,\n",
              "           1999,  1037,  2143,  1045,  1005,  1049,  2025,  1999,  1037,  2377,\n",
              "           1045,  2387,  2053, 12114,  2651,  1045,  2074,  2387,  2017,  1998,\n",
              "           2245,  1997,  2033, 30526,  1998,  2065,  1045,  2018,  2017,  2035,\n",
              "           1996,  3340,  2876,  1005,  1056,  2991,  2013,  1996,  3712,  1998,\n",
              "           1996,  4231,  2876,  1005,  1056,  2707,  2000,  5390,  2045,  1005,\n",
              "           1040,  2022,  2053, 17932,  1045,  1005,  1040,  2145,  2191, 12051,\n",
              "           2065,  1045,  2018,  2017,  2821,  2045,  1005,  1040,  2145,  2022,\n",
              "           2154,  1998,  2305,  1998,  1045,  1005,  1040,  2145,  2079,  3308,\n",
              "           1998,  2157,  1051, 11631,  2630,  2052,  2145,  2022,  2630,  2021,\n",
              "           2477,  2052,  2022,  6082,  2007,  2017, 30526,  1998,  2023,  2003,\n",
              "           2053,  4186,  1996,  2173,  2008,  1045,  2444,  1998,  1045,  2572,\n",
              "           2053,  2332,  2021,  1045,  1005,  2310,  2288,  2477,  2000,  2507,\n",
              "           1998,  1045,  5949,  2061,  2172,  2051,  3241,  1997,  2051,  1998,\n",
              "           1045,  2323,  2022,  2041,  2045,  6815,  2054,  1005,  1055,  3067,\n",
              "           2151,  2154,  1045,  2071,  3280,  2074,  2066,  1045,  2001,  2141,\n",
              "           1998,  2023,  2978,  1999,  1996,  2690,  2003,  2054,  1045,  1005,\n",
              "           1049,  2182,  2005,  1998,  1045,  2074,  2215,  2000,  6039,  2009,\n",
              "           2035,  2007,  6569, 30526,  1998,  2065,  1045,  2018,  2017,  2035,\n",
              "           1996,  3340,  2876,  1005,  1056,  2991,  2013,  1996,  3712,  1998,\n",
              "           1996,  4231,  2876,  1005,  1056,  2707,  2000,  5390,  2045,  1005,\n",
              "           1040,  2022,  2053, 17932,  1045,  1005,  1040,  2145,  2191, 12051,\n",
              "           2065,  1045,  2018,  2017,  2821,  2045,  1005,  1040,  2145,  2022,\n",
              "           2305,  1998,  2154,  1010,  1037,  1050,  2094,  2057,  1005,  1040,\n",
              "           2035,  2145,  2031,  2000,  3477,  1051, 11631,  2630,  2052,  2145,\n",
              "           2022,  2630,  2021,  2477,  2052,  2074,  2022,  6082,  2007,  2017,\n",
              "          30525, 30524,  2179,  2242,  6933,  1010,  2009,  2001,  2026,  3969,\n",
              "           1045,  7349,  2009,  6501,  2061,  2009,  2876,  1005,  1056,  4982,\n",
              "           2214,  2057,  4625,  1996,  3675,  4179,  2012,  6440,  1998,  8271,\n",
              "           2039,  1999,  1037,  2492,  1997,  9781,  1996,  7500,  2409,  2033,\n",
              "           1045,  2001,  2397,  1045,  2488,  2707,  3514,  2075,  1996,  4796,\n",
              "           2056,  2008,  2216,  2040,  5481,  2097,  2991,  2021,  1045,  2123,\n",
              "           1005,  1056, 10587,  3524,  2005,  5975,  1045,  2123,  1005,  1056,\n",
              "          10587,  3524,  2012,  2035,  2821,  1010,  8194,  1010,  2292,  1005,\n",
              "           1055,  2025,  3524,  2292,  1005,  1055,  2892,  1996,  2314,  2085,\n",
              "           2057,  2071,  4133,  2005,  2086,  4582,  2012,  2256, 10069,  2821,\n",
              "           1010,  2027,  1005,  2128,  2107,  3492,  2477,  2027,  1005,  2128,\n",
              "           2061, 10140,  2021,  2256,  5544,  2024,  2035,  2057,  2428,  2342,\n",
              "           2000,   102]),\n",
              "  'labels': 81.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  2113,  2179,  2242,  5996,  1010,  2009,  2001,  2026,  2422,\n",
              "           2009,  2018,  5295,  2993,  2000,  2305,  2061,  1045,  4711,  2009,\n",
              "           2041,  1037,  5645,  2240,  2056,  4608,  2115, 15934,  1998,  2059,\n",
              "           4608,  3067,  2821,  1010,  8194,  1010,  2292,  1005,  1055,  2025,\n",
              "           3524,  2292,  1005,  1055,  2892,  1996,  2314,  2085,  2057,  2071,\n",
              "           4133,  2005,  2086,  4582,  2012,  2256, 10069,  2821,  1010,  2027,\n",
              "           1005,  2128,  2107,  3492,  2477,  2027,  1005,  2128,  2061, 10140,\n",
              "           2021,  1999,  1996,  2203,  2027,  1005,  2128,  2074,  1037,  4848,\n",
              "           2821,  1010,  8194,  1010,  2292,  1005,  1055,  2025,  3524,  2051,\n",
              "           1005,  1055,  2025,  2006,  2256,  2217,  2092,  2009,  2196,  2001,\n",
              "           2057,  2113,  2008,  2784,  2503,  2821,  2074,  2298,  2012,  2017,\n",
              "           2007,  2115, 21766, 28579,  2606,  2821,  1045,  2293,  2017,  1998,\n",
              "           2008,  1005,  1055,  2035,  2017,  2342,  2000,  2113,  8194,  1010,\n",
              "           8194,  1012,  1012,  1012, 30525, 30524,  2065,  2035,  1996,  5344,\n",
              "           1999,  1996,  2088,  2018,  2431,  1997,  2115, 16056,  2431,  1997,\n",
              "           2115, 23210,  2059,  3407,  1045,  2052,  2022, 30525, 30524,  4133,\n",
              "           2091,  2026,  2293,  1997,  2035,  2204,  7258,  1998,  7200,  2026,\n",
              "           2293,  2292,  1005,  1055,  2035,  2022,  2985,  1045,  1005,  1049,\n",
              "           2183,  2000,  2425,  2017,  2119,  1037,  2466,  2055,  2070,  5758,\n",
              "           2008,  1045,  3959,  2102,  2156,  1045,  1005,  2310,  2439,  2293,\n",
              "           1999,  2116,  3182,  2025,  2560,  1996,  4534,  1997,  2624, 14174,\n",
              "           1998,  2814,  2926,  1010,  1045,  2134,  1005,  1056,  2191,  2068,\n",
              "           1037,  2702,  2723,  3332,  2000,  2014,  2188,  2823,  1045,  2071,\n",
              "           5390,  2005,  2661,  2823,  1045,  2071,  5390,  2005,  2661,  2823,\n",
              "           1045,  2071,  5390,  2005,  2661,  2021,  1045,  2123,  1005,  1056,\n",
              "           2061, 20261,  1996,  6281,  1999,  1996,  6116,  3392,  2022,  2145,\n",
              "           2017, 15390,  2588,  1996, 16808,  2026,  2540,  2038,  5357,  2011,\n",
              "           1996,  3971,  5178,  1045,  3685,  4366,  2014,  4902,  2061,  2123,\n",
              "           2115, 21019,  2884, 15695,  3243,  3254,  1998,  2023,  6517,  6467,\n",
              "           2038, 19128, 21737,  2094,  2096,  2006,  1996,  4534,  1997,  2214,\n",
              "           2624, 14174,  1045,  3422,  2026,  3336,  2108,  5296,  2823,  1045,\n",
              "           2071,  5390,  2005,  2661,  2823,  1045,  2071,  5390,  2005,  2661,\n",
              "           2823,  1045,  2071,  5390,  2005,  2661,  2021,  1045,  2123,  1005,\n",
              "           1056,  2823,  1045,  2071,  5390,  6289,  2823,  4530,  2026,  8641,\n",
              "           1998,  2448,  2005,  2661,  1998,  2823,  1045,  2071,  2444,  2026,\n",
              "           2166,  2021,  1045,  2180,  1005,  1056,  1010,  2021,  1045,  2180,\n",
              "           1005,  1056,  2031,  2017,  2412,  2042,  6908,  2408,  1996,  2300,\n",
              "           2031,  2017,  2412,  2042,  6908,  2408,  1996,  2300,  2031,  2017,\n",
              "           2412,  2042,  6908,  2408,  1996,  2300,  6229,  2045,  1005,  1055,\n",
              "           2053,  3096,  2187,  2006,  2115,  5944,  6908,  2408,  2300,  6908,\n",
              "           2408,  2300,  6908,  2408,  2300,  2066,  1037,  2962,  2131,  2033,\n",
              "           1037,  3460,  2131,  2033,  1037,  3460,  2040,  2097,  2131,  9436,\n",
              "           1997,  2026,  5944,  2131,  2033,  1037,  7089,  2131,  2033,  1037,\n",
              "           7089,  2040,  2097,  2681,  2026,  2132,  2894,  2131,  2033,  1037,\n",
              "           5268,  2131,  2033,  1037,  5268,  2040,  2097,  2954,  2033,  1999,\n",
              "           2023,  2162,  2131,  2033,  2019,  6164,  1045,  2342,  2019,  6164,\n",
              "           1045,  2342,  1037,  3332,  2030,  2341,  6908,  2408,  2300,  6908,\n",
              "           2408,  2300,  6908,  2408,  2300,  2066,  1037,  2962,  2131,  2033,\n",
              "           1037,  5160,  2131,  2033,  1037,  5160,  2040,  2097,  9790,  1996,\n",
              "           2088,  2005,  2033,  2131,  2033,  1037,  2711,  2131,  2033,  1037,\n",
              "           2711,  2131,  2033,  1037,  2711,  2040,  3475,  1005,  1056,  2033,\n",
              "          12731,   102]),\n",
              "  'labels': 81.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101, 30524,  2073,  2106,  2017,  2175,  2043,  1996,  4586,  3062,\n",
              "           2008,  2095,  1029,  2017,  2020,  2503,  2122,  4799,  3681,  2066,\n",
              "           1037,  4562,  9461,  2775,  2005,  1996,  2203,  1999,  2115, 21039,\n",
              "           2005,  1996,  2203,  1997,  1996,  2095,  1998,  2008,  2214,  5220,\n",
              "           3571, 19815,  2015,  2039,  2115,  2210,  2608,  1998,  3216,  2083,\n",
              "           2115,  9607,  2066,  2668,  2083,  2115,  2774,  2066,  2668,  2083,\n",
              "           2115,  2774,  2066,  2668,  2083,  2115,  2774,  2059,  2621,  2234,\n",
              "           1998,  2002,  5078,  2017,  2066,  1037, 11084,  1998,  2017,  2354,\n",
              "           4426,  1010,  2008,  2054,  2018,  2272,  2052,  2574,  2022,  2908,\n",
              "           2092,  2823,  1999,  2115, 22715, 15096,  4920,  2017,  2293,  2000,\n",
              "           2022,  1996, 19175,  2467,  3048,  2006,  2821,  2073,  2106,  1045,\n",
              "           2175,  2043,  1996, 15811,  3062,  2008,  2095,  1029,  2057,  2020,\n",
              "           2503,  2008,  4799,  2160,  2066,  1037,  4562,  9461,  2775,  2005,\n",
              "           1996,  2203,  2000,  2022,  2379,  2005,  1996,  2203,  1997,  1996,\n",
              "           2095,  1998,  2059,  3467,  2234,  1998,  1045,  5078,  2032,  2066,\n",
              "           1037, 11084,  1998,  2057,  2354,  2008,  2054,  2018,  2272,  2052,\n",
              "           2574,  2022,  2908,  2092,  2823,  1999,  2026, 22715, 15096,  4920,\n",
              "           1045,  2514,  2009,  2746,  2006,  1010,  2467,  3048,  2006,  2019,\n",
              "           2479,  2003,  2467,  2894,  1998,  2017,  2020,  4911,  6737,  2007,\n",
              "           1037,  2962,  2061,  1045,  2165,  2017,  2188,  2000,  1037, 26829,\n",
              "           1037,  7064,  2011,  2151,  2060,  2171,  2061,  1045,  2165,  2017,\n",
              "           2188,  1998,  1045,  8007,  2017,  1037,  7064,  1999,  2115,  9607,\n",
              "          30525, 30524,  2043,  2017,  2234,  2000,  2237,  1010,  1045,  3062,\n",
              "           2005,  2017,  1010,  2009,  3849,  2000,  1037,  2160,  2007,  6389,\n",
              "           5628,  1998,  1037, 13541,  3959,  1045,  1005,  2222,  2022,  2115,\n",
              "          21862, 22083,  3334,  3035,  2115, 21862, 22083,  3334,  3035,  2043,\n",
              "           1045,  2963,  2017,  2377,  1010,  1045,  2228,  1997,  2300,  1998,\n",
              "           9609,  1998,  1045,  1005,  2222,  2196,  2428,  2568,  2035,  2115,\n",
              "           2308, 15536,  3367,  3993,  5782,  2017,  2035,  1996,  2051,  1045,\n",
              "           1005,  2222,  2022,  2115, 21862, 22083,  3334,  3035,  2115, 21862,\n",
              "          22083,  3334,  3035,  2115, 21862, 22083,  3334,  3035,  1998,  9609,\n",
              "           2332, 30525, 30524,  2009,  1005,  1055,  1037,  3376,  2154,  1998,\n",
              "           1045,  2253,  2005,  1037,  3328,  2021,  1045,  2387,  2008,  2214,\n",
              "           6900,  4130,  2073,  2057,  2109,  2000,  2831,  2073,  2003,  1996,\n",
              "           4542,  1029,  1005,  3426,  2070,  3500, 10205,  2024,  5026,  2033,\n",
              "           2091,  2153,  1996,  2086,  2031,  2272,  1998,  2115,  2398,  2031,\n",
              "           2908,  2021,  2115,  2455,  2003,  2242,  2439,  1999,  1037,  2299,\n",
              "           2339,  2515,  1996,  3103,  2467, 10825,  2033,  1997,  2017,  1029,\n",
              "           2748,  2009,  2515,  1045,  3246,  1037,  2047,  2154,  2097,  2272,\n",
              "           1998,  3104,  2039,  2054,  1045,  2031,  2589,  2339,  2515,  1996,\n",
              "           3103,  2467, 10825,  2033,  1997,  2017,  1029,  1045,  2876,  1005,\n",
              "           1056,  2113,  2008,  2045,  7110,  1005,  1056,  2498,  2021,  2293,\n",
              "           2774,  2006,  1996,  2557,  2735,  2006,  1996,  6910,  2420,  2000,\n",
              "          14113,  2185,  1996,  3096,  1996,  3103,  2097,  2735,  2026,  2398,\n",
              "           2000,  2751,  2062,  2084,  2151,  2632, 24229,  2712,  2300,  1010,\n",
              "           2009,  6241,  1996,  3341,  1997, 21146,  4103,  1998,  4332,  2000,\n",
              "           5472,  1045,  2657,  2017,  4149,  1037,  2160,  1998,  2422,  2009,\n",
              "           2039,  2007,  3221,  1998,  2117, 11774,  2339,  2515,  1996,  3103,\n",
              "           2467, 10825,  2033,  1997,  2017,  1029,  2748,  2009,  2515,  1045,\n",
              "           3246,  1037,  2047,  2154,  2097,  2272,  1998,  3104,  2039,  2054,\n",
              "           1045,  2031,  2589,  2339,  2515,  1996,  3103,  2467, 10825,  2033,\n",
              "           1997,   102]),\n",
              "  'labels': 80.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  2017,  1029,  1045,  2876,  1005,  1056,  2113,  2008,  2045,\n",
              "           7110,  1005,  1056,  2498,  2021,  2293,  2774,  2006,  1996,  2557,\n",
              "           2467, 15537,  2033,  1997,  2017,  2748,  2009,  2515,  2009,  2467,\n",
              "          15537,  2033,  1997,  2017,  2748,  2009,  2515,  2748,  2009,  2515,\n",
              "           2748,  2009,  2515,  2339,  2515,  1996,  3103,  2467, 10825,  2033,\n",
              "           1997,  2017,  1029,  2748,  2009,  2515,  1045,  3246,  1037,  2047,\n",
              "           2154,  2097,  2272,  1998,  3104,  2039,  2054,  1045,  2031,  2589,\n",
              "           2339,  2515,  1996,  3103,  2467, 10825,  2033,  1997,  2017,  1029,\n",
              "           2748,  2009,  2515,  1045,  3246,  1037,  2047,  2154,  2097,  2272,\n",
              "           1998,  3104,  2039,  2054,  1045,  2031,  2589,  2339,  2515,  1996,\n",
              "           3103,  2467, 10825,  1997,  2017,  1029,  1045,  2876,  1005,  1056,\n",
              "           2113,  2008,  2045,  7110,  1005,  1056,  2498,  2021,  2293,  2774,\n",
              "           2006,  1996,  2557, 30525, 30524,  2720,  1012,  2198,  3389,  1010,\n",
              "           2054,  2003,  6230,  2000,  2033,  1029,  2197,  2051,  1045,  2387,\n",
              "           2017,  5026,  2543,  3702,  2046,  2115,  2160,  2013,  1037,  3392,\n",
              "           2021,  1045,  2134,  1005,  1056,  2293,  2017,  3243,  2004,  2172,\n",
              "           2004,  4086,  5032,  6458,  1010,  2002,  2196,  2018,  2000,  2272,\n",
              "           2000, 10653,  2083,  1996,  2300, 22722,  1998,  2070,  4326,  6547,\n",
              "           2000,  2424,  5032,  2033, 13681,  1999, 10966, 12129,  1010,  2417,\n",
              "           2970,  1010,  1998,  2152,  1011, 12073,  2098,  6007,  1045,  3344,\n",
              "           2017,  1010,  2115,  3635,  2005,  1037,  2146,  2051,  2083,  2086,\n",
              "           1998,  4865,  1998,  2165,  9277,  1998,  1045,  3344,  2115,  3635,\n",
              "           2588,  3067,  5032,  2288,  2496,  2000,  1037,  2158,  1998,  1045,\n",
              "           2179,  2115,  9717,  1999,  2621,  4915,  2000,  2115,  3203,  5032,\n",
              "           2038,  1037,  2684,  1998,  1037,  2047,  2103,  2085,  1998,  2115,\n",
              "           3096,  2003,  3730,  1998,  5458,  2013,  1996,  2086,  2017,  1005,\n",
              "           2128,  3407,  2085,  2033, 13681,  1999, 10966, 12129,  1010,  2417,\n",
              "           2970,  1010,  1998,  2152,  1011, 12073,  2098,  6007,  1045,  3344,\n",
              "           2017,  1010,  2115,  3635,  2005,  1037,  2146,  2051,  2083,  2086,\n",
              "           1998,  4865,  1998,  2165,  9277,  1998,  1045,  3344,  2115,  3635,\n",
              "           2588,  3067,  2720,  1012,  2198,  3389,  1010,  2054,  2003,  6230,\n",
              "           2000,  2033,  1029,  2197,  2051,  1045,  2387,  2017,  5026,  2543,\n",
              "           3702,  2046,  2115,  2160,  2013,  1037,  3392, 30525, 30524,  3336,\n",
              "           1010,  1045,  2097,  2681,  2017,  1999,  1996,  2851, 11559,  2154,\n",
              "           1010,  1045,  2180,  1005,  1056,  2113,  2054,  2000,  2360,  4097,\n",
              "           1997,  3103,  1998,  2420,  1997,  2770,  2754,  4597,  1045,  2123,\n",
              "           1005,  1056,  2428,  2113,  2178,  2126,  3336,  1010,  2017,  2342,\n",
              "           2000,  2275,  2033,  2489,  3336,  1010,  2017,  1005,  2222,  2342,\n",
              "           2000,  2275,  2033,  2489,  2662, 10749,  6229,  1045,  2071,  3637,\n",
              "           2157,  2070,  2020,  4326,  1998,  2008,  1045,  3685,  4682,  4763,\n",
              "           2001,  1996,  2158,  2008,  1045,  2106,  2444,  2011,  2053,  4392,\n",
              "           2053,  2028,  2071,  2191,  2033,  2272,  4142,  6861,  1010,  1045,\n",
              "           2123,  1005,  1056,  2215,  2000,  2175,  2185,  6861,  1010,  1045,\n",
              "           2123,  1005,  1056,  2215,  2000,  2175,  2185,  3336,  1010,  2097,\n",
              "           2017,  2293,  2033,  2043,  1045,  2709,  1029,  4872,  1045,  2097,\n",
              "           2907,  2017,  1999,  2026,  5340,  2296,  2305,  1010,  1045,  3959,\n",
              "           2017,  5777,  2279,  2000,  2033,  6170,  1037,  2299,  1998,  2562,\n",
              "           2017,  2066,  1037, 28352,  2213,  3336,  1010,  2017,  2097,  2342,\n",
              "           2000,  9641,  2033,  2042,  1037, 26403,  2121,  2035,  2026,  2166,\n",
              "           1010,  2017,  2156,  2047,  2259,  2103, 10749,  6229,  1045,  2071,\n",
              "           3637,  2157,  2279,  2000,  2028,  1045,  5621,  2245,  1045,  3866,\n",
              "           6057,   102]),\n",
              "  'labels': 80.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  2129,  1996,  2051,  2064,  2735,  2115, 18167,  9716,  2066,\n",
              "           1037,  2543,  2013,  2682,  3336,  1010,  2017,  2097,  2342,  2000,\n",
              "           9641,  2033,  2042,  1037, 26403,  2121,  2035,  2026,  2166,  1010,\n",
              "           2017,  2156,  1045,  2572,  2893,  3020,  2011,  1996,  2617,  2420,\n",
              "           2205,  2146,  1998,  6385,  2205,  3147,  1999,  2299,  1045,  2572,\n",
              "           2893,  3020,  2011,  1996,  2617,  1045,  2001,  3308,  2000,  2681,\n",
              "           2017,  1010,  1045,  2001,  3308, 30525, 30524,  2320,  2115,  2159,\n",
              "           2020,  1996,  3040, 18437,  2000,  2035,  2026,  2774, 25627,  1998,\n",
              "           2712,  2272,  2067,  2000,  2033,  1045,  1005,  2222,  2196,  2079,\n",
              "           2017,  3308,  3467,  2052,  2272,  1998,  2498,  2052,  2693,  2621,\n",
              "           1005,  1055, 25757,  2007,  2035,  2106,  1045,  5454, 25627,  1998,\n",
              "           2712,  2272,  2067,  2000,  2033,  1045,  1005,  2222,  2196,  2079,\n",
              "           2017,  3308,  1045,  2001,  2115,  7247,  1998,  1045,  2354,  2008,\n",
              "           2017,  9828,  1045,  2001,  2115,  7247,  1998,  1045,  2354,  2008,\n",
              "           2017,  9828,  9906,  2035,  1045,  2215,  2003,  2017,  2823,  1045,\n",
              "           2903,  1045,  2079,  5595,  7817,  1997, 27158,  1998, 25238, 13997,\n",
              "           3040,  1010,  2156,  2033,  2083,  9906,  2035,  1045,  2215,  2003,\n",
              "           2017, 13997,  3040,  1010,  2156,  2033,  2083, 25806,  1999,  2192,\n",
              "           1010,  1037,  5442,  2012,  2026,  7817,  2019,  5976,  2126,  2000,\n",
              "           2175,  1010,  2021,  1045,  2464,  7985,  2477,  2821,  1045,  3473,\n",
              "           1996, 10418,  4777,  2058,  1996,  2300,  1998,  2006,  2026,  2214,\n",
              "           2273, 17325,  2666,  1037,  5835,  2008,  2017,  2081,  2768,  2033,\n",
              "          25627,  1998,  2712,  2272,  2067,  2000,  2033,  1045,  1005,  2222,\n",
              "           2196,  2079,  2017,  3308,  1045,  2001,  2115,  7247,  1998,  1045,\n",
              "           2354,  2008,  2017,  9828,  1045,  2001,  2115,  7247,  1998,  1045,\n",
              "           2354,  2008,  2017,  9828,  9906,  2035,  1045,  2215,  2003,  2017,\n",
              "           2823,  1045,  2903,  1045,  2079,  5595,  7817,  1997, 27158,  1998,\n",
              "          25238, 13997,  3040,  1010,  2156,  2033,  2083,  9906,  2035,  1045,\n",
              "           2215,  2003,  2017, 13997,  3040,  1010,  2156,  2033,  2083, 30525,\n",
              "          30524,  3612,  2014,  2039,  1010,  3612,  2014,  2039,  2009,  1005,\n",
              "           1055,  2042,  2086,  2144,  2016,  2234,  2058,  2006,  1996,  2665,\n",
              "           8575,  2000,  2017,  2016,  4455,  2010,  2171, 21146,  1011,  6583,\n",
              "           1011,  9266,  1037,  9266,  1037,  9266,  1037,  9266,  2058,  2600,\n",
              "           1998,  2712,  2000,  2014,  3336,  2043,  1996,  3482,  2253,  1999,\n",
              "           1996,  2598, 11354,  3628,  2020,  2035,  2105,  1037,  2162,  2106,\n",
              "           2202,  2032,  2013,  2014,  2237,  1998,  2016,  7110,  1005,  1056,\n",
              "           2498,  2021,  1037,  5745,  2085,  3612,  1011,  2039, 10658,  1999,\n",
              "           1996,  2067,  3612,  1011,  2039, 10658, 11562,  1998, 18856,  8684,\n",
              "          11562,  1998, 18856,  8684,  2016,  2097,  2196,  2022,  2054,  2017,\n",
              "           2215,  2014,  2000,  2022,  2016,  2038,  5544,  2296,  2305,  2008,\n",
              "           1996,  3482,  1997, 11354,  2073,  2073,  2017,  2253,  2000,  3913,\n",
              "           2097,  2272,  2000,  2166,  2907,  2014,  2006,  1996, 15505,  2907,\n",
              "           2014,  2006,  1996,  2665,  8575,  2066,  2017,  2109,  2000,  2079,\n",
              "           2021,  2016,  7110,  1005,  1056,  2498,  2021,  1037,  5745,  2017,\n",
              "           2017,  3612,  2014,  2039,  1999,  1996,  2067,  3612,  1011,  2039,\n",
              "          10658, 11562,  1998, 18856,  8684, 11562,  1998, 18856,  8684,  2016,\n",
              "           2097,  2196,  2022,  2054,  2017,  2215,  2014,  2000,  2022,  3612,\n",
              "           2014,  2039,  2000,  3328,  1998,  3612,  2014,  2039,  2000,  3713,\n",
              "           2907,  2014,  2006,  1996, 15505,  2907,  2014,  2006,  1996,  2665,\n",
              "           8575,  2066,  2017,  2109,  2000,  2079,  2021,  2016,  7110,  1005,\n",
              "           1056,  2498,  2021,  1037,  5745, 30525, 30524,  2043,  2017,  2455,\n",
              "           1010,   102]),\n",
              "  'labels': 80.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  1045,  1005,  2222,  2022,  2105,  1045,  1005,  2222,  3524,\n",
              "           2005,  2017,  2000,  2272,  2105,  2043,  2017,  2455,  1010,  1045,\n",
              "           1005,  2222,  2022,  2105,  1045,  1005,  2222,  3524,  2005,  2017,\n",
              "           2000,  2272,  2105,  2507,  2033,  1037,  3382,  1010,  1045,  1005,\n",
              "           2222,  2191,  2017,  1037,  3614,  1998,  2057,  1005,  2222,  4875,\n",
              "           2000,  2256,  5030,  2507,  2033,  1037,  3382,  1010,  1045,  1005,\n",
              "           2222,  2191,  2017,  6170,  1998,  2057,  1005,  2222,  4875,  2000,\n",
              "           2256,  5030,  2043,  2017,  4125,  1010,  1045,  1005,  2222,  2022,\n",
              "           2105,  1045,  1005,  2222,  3524,  2005,  2017,  2000,  2272,  2105,\n",
              "           1998,  2043,  2017,  2455,  1010,  1045,  1005,  2222,  2022,  2105,\n",
              "           1045,  1005,  2222,  3524,  2005,  2017,  2000,  2272,  2105,  2507,\n",
              "           2033,  1037,  3382,  1010,  1045,  1005,  2222,  2191,  2017,  1037,\n",
              "           3614,  1998,  2057,  1005,  2222,  4875,  2000,  2256,  5030,  2507,\n",
              "           2033,  1037,  3382,  1010,  1045,  1005,  2222,  2191,  2017,  1037,\n",
              "           3614,  1998,  2057,  1005,  2222,  4875,  2000,  2256,  5030, 30525,\n",
              "          30524,  2210,  2332,  1010,  2017,  3473,  2039,  1037,  3542,  2013,\n",
              "           2026,  2237,  1998,  1045,  2196,  2245,  2000,  4339,  2009,  2091,\n",
              "          10958,  6834,  1996,  7114,  3727,  1998,  5327,  2005,  2070,  3492,\n",
              "           2194,  1045,  2001,  2467, 14743,  1037, 15940,  1999,  1996,  2601,\n",
              "           1999,  2026,  2568,  1010,  1037,  3307,  2000,  1996,  2530, 12125,\n",
              "           2069,  2019,  2058, 15194,  2058,  1996,  4564,  1997, 12727,  5568,\n",
              "           1998,  1045,  2052,  2031,  2579,  2035,  2115,  9494,  1998,  1045,\n",
              "           2052,  2031,  2579,  2035,  2115,  6331,  2015,  1998,  1045,  2052,\n",
              "           2031,  2579,  2035,  2115,  9494,  2005,  1037,  4536,  2210,  2332,\n",
              "           1010,  1045,  8415,  2008,  2023,  2003,  2025,  1037,  3959,  1045,\n",
              "           3191,  2008,  2023,  2003,  2054,  2009,  2323,  2022,  1999,  1037,\n",
              "           2932,  2365,  1997,  1996,  3714,  1011,  2091,  1010,  2684,  1997,\n",
              "           1037,  2293,  1998,  3827,  2237,  1998,  1045,  2052,  2031,  2579,\n",
              "           2035,  2115,  9494,  1998,  1045,  2052,  2031,  2579,  2035,  2115,\n",
              "           6331,  2015,  1998,  1045,  2052,  2031,  2579,  2035,  2115,  9494,\n",
              "           2005,  1037,  4536,  2005,  1037,  4536,  2005,  1037,  4536,  2057,\n",
              "           2064,  4392, 13803,  2030,  4133,  2503,  1996,  2482,  1998,  4952,\n",
              "           2000,  1996,  4223,  1997,  1996,  3340,  2069,  2019,  2058, 15194,\n",
              "          22057,  2013,  4564,  1997,  3714,  3221,  2116,  4915,  1997, 14038,\n",
              "           1998, 10261,  2005,  4826,  2191,  2033,  2115,  4086,  1011, 18627,\n",
              "           3203, 30525, 30524,  1999,  1037,  2932,  1045,  2387,  2017,  4911,\n",
              "           2091,  1045,  2387,  2017,  4911,  2091,  1998,  1045,  3858,  2115,\n",
              "           2227,  2017,  2298,  2066,  2619,  2008,  1045,  2109,  2000,  2113,\n",
              "           2026,  2398,  2024,  5079,  1998,  1996,  4542,  4212,  2026,  2398,\n",
              "           2024,  5079,  1998,  1996,  4542,  4212,  2017,  2298,  2066,  2619,\n",
              "           2008,  1045,  2109,  2000,  2113,  2017,  2298,  2066,  2619,  2008,\n",
              "           1045,  2109,  2000,  2113,  7650,  1996,  5108,  3342,  2035,  2216,\n",
              "           8484,  2057,  7147,  1996,  3712,  2000,  2317,  2004,  1996,  2422,\n",
              "           9938, 15999,  1999,  2021,  1996,  2051,  1010,  2009, 11055,  1996,\n",
              "           3609,  2013,  2115,  3096,  2026,  2398,  2024,  5079,  1998,  1996,\n",
              "           4542,  4212,  2026,  2398,  2024,  5079,  1998,  1996,  4542,  4212,\n",
              "           2017,  2298,  2066,  2619,  2008,  1045,  2109,  2000,  2113,  2017,\n",
              "           2298,  2066,  2619,  2008,  1045,  2109,  2000,  2113,  1999,  1037,\n",
              "           2932,  1045,  2387,  2017,  4911,  2091,  1998,  1045,  3858,  2115,\n",
              "           2227,  2017,  2298,  2066,  2619,  2008,  1045,  2109,  2000,  2113,\n",
              "           2017,  2298,  2066,  2619,  2008,  1045,  2109,  2000,  2113,  2017,\n",
              "           2298,   102]),\n",
              "  'labels': 80.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101, 30524, 30526,  2298,  1999,  1996,  5259,  2040,  2079,  2017,\n",
              "           2156,  1029,  2619,  5220,  2021,  7543,  2025,  2033,  2005,  2002,\n",
              "           2064,  1005,  1056,  2022,  2033,  2298,  2129,  2214,  1998,  3147,\n",
              "           1998,  5458,  1998,  9479,  2002,  1005,  1055,  2468,  2025,  2127,\n",
              "           2017,  2156,  2045,  1005,  1055,  1037,  3976,  6415,  5689,  2125,\n",
              "           1997,  2383,  2035,  2008,  4569, 30526,  7910,  1011,  2821,  1010,\n",
              "           3504,  2066,  1045,  1005,  1049,  3773,  2062,  1997,  2026,  2214,\n",
              "           2158,  1999,  2033,  2821,  2053,  1010,  3504,  2066,  1045,  1005,\n",
              "           1049,  3773,  2062,  1997,  2026,  2214,  2158,  1999,  2033, 30526,\n",
              "           3328,  2006,  1996,  2648,  1010,  3173,  2014,  2192,  2619,  5220,\n",
              "           1010,  2014,  1998,  2014,  2158,  2021,  2009,  2074,  2064,  1005,\n",
              "           1056,  2022,  2298,  2012,  2035,  1996,  4084,  2008,  2716,  2017,\n",
              "           2073,  2017,  2024,  2651,  2025,  2127,  2017,  2156,  2004,  1037,\n",
              "           2540,  7502,  6428,  1010,  2823,  2293,  2003,  3724,  2185, 30526,\n",
              "           7910,  1011,  2821,  1010,  3504,  2066,  1045,  1005,  1049,  3773,\n",
              "           2062,  1997,  2026,  2214,  2158,  1999,  2033,  2821,  2053,  1010,\n",
              "           3504,  2066,  1045,  1005,  1049,  3773,  2062,  1997,  2026,  2214,\n",
              "           2158,  1999,  2033,  7910,  1011,  2821,  1010,  3504,  2066,  1045,\n",
              "           1005,  1049,  3773,  2062,  1997,  2026,  2214,  2158,  1999,  2033,\n",
              "           2821,  2053,  1010,  3504,  2066,  1045,  1005,  1049,  3773,  2062,\n",
              "           1997,  2026,  2214,  2158,  1999,  2033, 30525, 30524, 30526,  2823,\n",
              "           2026,  2293,  2089,  2022,  2404,  2006,  2907,  2823,  2026,  2540,\n",
              "           2089,  4025,  9643,  3147,  2122,  2335,  2272,  1998,  2122,  2335,\n",
              "           2175,  2004,  2146,  2004,  1045,  2444,  1010,  2035,  2017,  2342,\n",
              "           2000,  2113,  2003, 30526,  2023,  2214,  3899,  7110,  1005,  1056,\n",
              "           2055,  2000,  5293,  2035,  2057,  1005,  2310,  2018,  1010,  1998,\n",
              "           2035,  2008,  1005,  1055,  2279,  1005,  2146,  2004,  2026,  2540,\n",
              "           1005,  1055,  3786,  2378,  1005,  1999,  2026,  3108,  2023,  2214,\n",
              "           3899,  7110,  1005,  1056,  2055,  2000,  5293, 30526,  2411,  1037,\n",
              "           2540, 12102,  2000,  2689,  2049,  2568,  1037,  2047,  2154,  7288,\n",
              "           2006,  1037,  2047,  2640,  1037,  2047,  2154,  4152,  2275,  2006,\n",
              "           2178,  2126,  2004,  2146,  2004,  1045,  2444,  1010,  2035,  1045,\n",
              "           1005,  2310,  2288,  2000,  2360,  2003, 30526,  2023,  2214,  3899,\n",
              "           7110,  1005,  1056,  2055,  2000,  5293,  2035,  2057,  1005,  2310,\n",
              "           2018,  1010,  1998,  2035,  2008,  1005,  1055,  2279,  1005,  2146,\n",
              "           2004,  2026,  2540,  1005,  1055,  3786,  2378,  1005,  1999,  2026,\n",
              "           3108,  2023,  2214,  3899,  7110,  1005,  1056,  2055,  2000,  5293,\n",
              "          30525, 30524, 30526,  1997,  2035,  1996,  2477,  2017,  6257,  2017,\n",
              "           1005,  1040,  2196,  2589,  1045,  1005,  2310,  2657,  2017,  2360,\n",
              "           2008,  2023,  2003,  2193,  2028,  2178,  2166,  2091,  2178,  2346,\n",
              "           3383,  2017,  1005,  2128,  2488,  2125,  2000,  2196,  2113, 30526,\n",
              "           2123,  1005,  1056,  3959,  1997,  2035,  1996,  3971,  2477,  2071,\n",
              "           2031,  2042,  3342,  2035,  1996,  5344,  2017,  2292,  1999,  3342,\n",
              "           2035,  1996,  5344,  2145, 16100,  1998,  2574,  2438,  1010,  2017,\n",
              "           1005,  2222,  2156,  2074,  2054,  1045,  2812, 30526,  2045,  1005,\n",
              "           1055,  2498,  2000,  5390,  2055,  1005,  3426,  3336,  1010,  3336,\n",
              "           1010,  2017,  1005,  2128,  2041, 30526,  2017,  1005,  2310,  2467,\n",
              "           2371,  2009,  1005,  1055,  2524,  2000,  2376, 10821,  2021,  2054,\n",
              "           2017,  2376,  2003,  2054,  2097,  3298,  2115,  6580,  1998,  2295,\n",
              "           2477,  2196,  2428,  2288,  2008,  2919,  5544,  1997, 11006,  2099,\n",
              "           5568,  1005,  2222,  3298,  2017,  5506, 30526,  2123,  1005,  1056,\n",
              "           3959,   102]),\n",
              "  'labels': 79.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  1997,  2035,  1996,  3971,  2477,  2071,  1005,  2310,  2042,\n",
              "           3342,  2035,  1996,  5344,  2017,  2292,  1999,  3342,  2035,  1996,\n",
              "           5344,  2145, 16100,  1998,  2574,  2438,  1010,  2017,  1005,  2222,\n",
              "           2156,  2074,  2054,  1045,  2812, 30526,  2045,  1005,  1055,  2498,\n",
              "           2000,  5390,  2055,  1005,  3426,  3336,  1010,  3336,  1010,  2017,\n",
              "           1005,  2128,  2041, 30526,  3336,  1010,  2017,  1005,  2128,  2041,\n",
              "           3336,  1010,  2017,  1005,  2128,  2041,  3336,  1010,  2017,  1005,\n",
              "           2128,  2041, 30525, 30524, 30526,  2096,  2016,  1005,  1055,  2042,\n",
              "           2185,  2542,  2154,  1011,  2000,  1011,  2154,  2038,  2042,  7823,\n",
              "           2302,  2014,  2012,  2026,  2217,  3432,  2108,  4142,  2038,  2042,\n",
              "           5931, 30526,  1998,  2295,  2016,  2180,  1005,  1056,  2022,  2908,\n",
              "           5091,  2045,  2024,  2116,  2335,  1045,  2424,  2009,  5683,  2008,\n",
              "           2126,  1998,  1045,  1005,  1049,  2025,  2667,  2000,  5293,  2014,\n",
              "           2074,  3305,  2129,  1045,  1005,  2222,  2022,  3110,  2006,  2008,\n",
              "           2154, 30526,  2009,  1005,  1055,  2074,  2066,  3773,  2014,  2005,\n",
              "           1996,  2034,  2051,  2153,  2009,  1005,  1055,  2074,  2066,  3773,\n",
              "           2014,  2005,  1996,  2034,  2051,  2153, 30526,  1996,  2051,  2016,\n",
              "           1005,  1055,  2025,  2105,  4873,  2041,  1997,  2237,  1010,  2038,\n",
              "           2042,  2524,  2021,  5064,  2023,  2214,  2540,  2179,  2051,  2000,\n",
              "           2147,  2009,  2041,  2023,  2521, 30526,  2053,  1010,  1045,  2113,\n",
              "           2016,  1005,  1055,  2746,  2188,  2574,  2045,  2024,  2335,  1045,\n",
              "           2424,  2009,  2524,  2000,  2514,  2008,  2126,  2009,  1005,  1055,\n",
              "           2025,  2503,  2033,  2000,  5293,  2014,  2074,  3305,  2129,  1045,\n",
              "           1005,  2222,  2022,  3110,  2006,  2008,  2154, 30526,  2009,  1005,\n",
              "           1055,  2074,  2066,  3773,  2014,  2005,  1996,  2034,  2051,  2153,\n",
              "           2009,  1005,  1055,  2074,  2066,  3773,  2014,  2005,  1996,  2034,\n",
              "           2051,  2153,  2009,  1005,  1055,  2074,  2066,  3773,  2014,  2005,\n",
              "           1996,  2034,  2051,  2153,  2009,  1005,  1055,  2074,  2066,  3773,\n",
              "           2014,  2005,  1996,  2034,  2051,  2153, 30525, 30524, 30526,  4931,\n",
              "           4845,  1010,  7955,  1005,  1055, 13047,  2000,  2070, 12051,  2065,\n",
              "           2017,  1005,  1040,  2467,  2921,  2009,  3442,  1010,  2017,  1005,\n",
              "           1040,  2196,  4553,  2017,  1005,  1040,  2448,  1996,  3891,  1997,\n",
              "           2035,  1996, 10831,  2017,  2202,  2123,  1005,  1056,  2514,  2066,\n",
              "           2035,  1996,  2051,  2017,  2404,  1999,  2253,  2000,  5949,  1996,\n",
              "           2126,  2115,  2540,  2001,  6012,  2035,  2216,  2420,  1998,  3402,\n",
              "           2009, 10299,  2178,  6393, 30526,  2009,  1005,  1055,  2025,  2066,\n",
              "           2017,  1005,  1040,  2412,  3046,  2000,  5293,  2014,  2021,  2122,\n",
              "           2420,  2024,  2488,  2302,  2028,  2178, 30526,  4931,  2158,  1010,\n",
              "           2061,  2085,  2017,  1005,  2310,  2288,  2009,  2125,  2115,  3108,\n",
              "           2115,  2540,  2064,  2633,  2131,  2070,  2717,  2168,  2540,  2008,\n",
              "           2318,  2023,  2878,  6752,  1998,  2348,  1010,  1037,  3714,  2540,\n",
              "           3791,  2051,  2000,  2273,  2094,  6737,  2347,  1005,  1056,  2412,\n",
              "           2428,  3714,  1010,  2001,  2009,  1029,  2017,  4558,  1037,  2293,\n",
              "           1010,  2017,  5114,  1037,  2767, 30526,  2009,  1005,  1055,  2025,\n",
              "           2066,  2017,  1005,  1040,  2412,  3046,  2000,  5293,  2014,  2021,\n",
              "           2122,  2420,  2024,  2488,  2302,  2028,  2178, 30526,  2302,  2028,\n",
              "           2178,  2302,  2028,  2178,  2302,  2028,  2178, 30525, 30524, 30526,\n",
              "           1045,  2196,  3214,  2000,  2191,  2014,  5390,  1998,  2085,  1045,\n",
              "           1005,  1049,  3773,  4000,  1999,  2014,  2159,  2431,  1998,  2431,\n",
              "           1010,  2191,  2903,  1005,  3426,  2061,  2016,  2245,  1010,  2092,\n",
              "           1010,  2026,  2540,  2001,  2006,  2026, 10353, 30526,  6861,  1010,\n",
              "           1045,   102]),\n",
              "  'labels': 79.0},\n",
              " {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1]),\n",
              "  'input_ids': tensor([  101,  6639,  2205,  2017,  2488,  2903,  2009,  6861,  1010,  2026,\n",
              "           2540,  2145, 10299,  2005,  2017,  2130,  2295,  2017,  2123,  1005,\n",
              "           1056,  2514,  2009,  6012, 30526,  2196,  2245,  2070, 10021,  2774,\n",
              "           2071,  2412,  2175,  1998,  3480,  2619,  1045,  2196,  3214,  2000,\n",
              "           6170,  2026,  8694,  2005, 10334,  2842,  2041,  2045,  2021,  2017,\n",
              "          30526,  6861,  1010,  1045,  6639,  2205,  2017,  2488,  2903,  2009,\n",
              "           6861,  1010,  2026,  2540,  2145, 10299,  2005,  2017,  2130,  2295,\n",
              "           2017,  2123,  1005,  1056,  2514,  2009,  6861,  1010,  1045,  6639,\n",
              "           2205,  2017,  2488,  2903,  2009,  6861,  1010,  2026,  2540,  2145,\n",
              "          10299,  2005,  2017,  2130,  2295,  2017,  2123,  1005,  1056,  2514,\n",
              "           2009,  6012, 30525, 30524, 30526,  4332,  2041,  2025,  2296,  3899,\n",
              "           2038,  2010,  2154,  2061,  6517,  1010,  2061,  3402,  2908,  2185,\n",
              "           4299,  2045,  2020,  2062,  2008,  1045,  2071,  2079,  2151,  2051,\n",
              "           2017,  1005,  2128,  4994,  2023,  2905,  1010,  2113,  2026,  2540,\n",
              "           3632,  2041,  2000,  2017, 30525, 30524, 30526,  2320,  2115,  5544,\n",
              "           2272, 10591,  2012,  2115,  2341,  2009,  1005,  1055,  2051,  2000,\n",
              "           5382,  2017,  4995,  1005,  1056, 12802,  4902,  1998,  2320,  2115,\n",
              "           2166,  2003,  2275,  2000,  7392,  2091,  2202,  1037,  2298,  2105,\n",
              "           2017,  2053,  2062, 12802,  2000,  2022,  2179, 30526,  2061,  2339,\n",
              "           1010,  2059,  1010,  2024,  2017,  6933,  1029,  2009,  2001,  2017,\n",
              "           2040,  6380,  2068,  1998,  2053,  3815,  1997,  4000,  2071,  4897,\n",
              "           2067,  2035,  1996,  2086,  3288,  2067,  2035,  2115,  5544,  2013,\n",
              "           7483, 30526,  2320,  1037,  2166,  7164,  2009,  1005,  1055,  2288,\n",
              "           2009,  2275,  2039,  1037,  3553,  2298,  7657,  2074,  2129,  4064,\n",
              "           2017,  2064,  2514,  2320,  1037,  3959,  2003,  2633,  2404,  2000,\n",
              "           2793,  2717,  2039,  1010, 17056,  4974,  2017,  2453,  2004,  2092,\n",
              "           2022,  2757, 30526,  2061,  2339,  1010,  2059,  1010,  2024,  2017,\n",
              "           6933,  1029,  2009,  2001,  2017,  2040,  6380,  2068,  1998,  2053,\n",
              "           3815,  1997,  4000,  2071,  4897,  2067,  2035,  1996,  2086,  3288,\n",
              "           2067,  2035,  2115,  5544,  2013,  7483, 30525, 30524, 30526,  2074,\n",
              "           2667,  2000,  2562,  2009,  2422,  2823, 23942,  1037,  5192,  4390,\n",
              "           2003,  4363,  4251,  1998,  2017,  2113,  2092,  2073,  2115,  2540,\n",
              "           3632, 13460,  2007,  3116, 10334,  1010,  2122,  2420,  2024,  5026,\n",
              "           2017,  2091,  2035,  3653, 10521, 19155,  2000,  2903,  1999,  1010,\n",
              "           1999,  1037,  4702,  2040, 11651,  8351,  1005,  1055,  4253,  3398,\n",
              "          30526,  2123,  1005,  1056,  2292,  1996,  2088,  2648,  1996,  3332,\n",
              "           9739,  2063,  2131,  2000,  2115,  2132, 11504,  2191,  2070,  3168,\n",
              "           1997,  2035,  2023,  4485,  2077,  2017,  1005,  2128,  2757,  1996,\n",
              "          13460,  2007,  3116, 10334,  1010,  3582, 11210,  2017,  2175,  2035,\n",
              "           3653, 10521, 19155,  2000,  2903,  1999,  1010,  1999,  1037,  4702,\n",
              "           2040, 11651,  8351,  1005,  1055,  4253,  3398, 30525, 30524, 30526,\n",
              "           2178,  2735,  1010,  2178,  5670,  2028,  2197,  3610,  2013,  2014,\n",
              "           2970,  2178,  3959,  2017,  1005,  2128,  5128,  2091,  2044,  2035,\n",
              "           2023,  2051,  1010,  2009,  4332,  2041,  2035,  2017,  2179, 30526,\n",
              "           2003,  2028,  2062,  2293,  2041,  2000,  3338,  2115,  2540,  2275,\n",
              "           2009,  2039,  2074,  2000,  3422,  2009,  2991,  4237, 30526,  2178,\n",
              "           3046,  1010,  2178,  2175,  2196,  2245,  2017,  1005,  1040,  2514,\n",
              "           2023,  2659,  2178,  3959,  2404,  2000,  2793,  2044,  2035,  2023,\n",
              "           2051,  1010,  2009,  4332,  2035,  2017,  2018, 30526,  2003,  2028,\n",
              "           2062,  2293,  2041,  2000,  3338,  2115,  2540,  2275,  2009,  2039,\n",
              "           2074,  2000,  3422,  2009,  2991,  4237,  2028,  2062,  2293,  2041,\n",
              "           2000,   102]),\n",
              "  'labels': 79.0}]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# methodology = 1\n",
        "# methodology_name = methodologies[methodology][0]\n",
        "# create_encodings_fx = methodologies[methodology][1]\n",
        "# make_datasets_fx = methodologies[methodology][2]\n",
        "# standardize_parts = methodologies[methodology][3]\n",
        "# see_lbs = methodologies[methodology][4]\n",
        "# chunk = methodologies[methodology][5]\n",
        "\n",
        "# print(f\"Running following model: {methodology_name}\")\n",
        "# print(f\"Methodology: {methodology_name}\")\n",
        "# print(f\"Create encodings function: {create_encodings_fx}\")\n",
        "# print(f\"Standardize parts: {standardize_parts}\")\n",
        "# print(f\"See line breaks: {see_lbs}\")\n",
        "# print(f\"Chunking: {chunk}\")\n",
        "\n",
        "# train_dataset, valid_dataset = create_datasets(tokenizer, create_encodings_fx, make_datasets_fx, standardize_parts, see_lbs)\n",
        "\n",
        "my_batch = [train_dataset[i] for i in range(3)]\n",
        "\n",
        "def chunker(item, chunksize):\n",
        "    newObs = []\n",
        "    input_id_chunks = list(item['input_ids'].split(chunksize - 2))\n",
        "    mask_chunks = list(item['attention_mask'].split(chunksize - 2))\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        if input_id_chunks[i][-1].item() == 0:\n",
        "            break\n",
        "        input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "        # add attention tokens to attention mask\n",
        "        mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "        # get required padding length\n",
        "        pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "        # check if tensor length satisfies required chunk size\n",
        "        if pad_len > 0:\n",
        "            # if padding length is more than 0, we must add padding\n",
        "            input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "            mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "        newDictItem = {}\n",
        "        newDictItem['input_ids'] = input_id_chunks[i]\n",
        "        newDictItem['attention_mask'] = mask_chunks[i]\n",
        "        newDictItem['labels'] = item['labels']\n",
        "        newObs.append(newDictItem)\n",
        "    return newObs\n",
        "\n",
        "def collate_batch_into_chunks(features):\n",
        "    newObs = []\n",
        "    for item in features:\n",
        "        newObs.extend(chunker(item, 512))\n",
        "    \n",
        "    print(len(newObs))\n",
        "    return newObs\n",
        "\n",
        "# collate_batch_into_chunks(my_batch)\n",
        "\n",
        "# print(\"Finalized dataset creation, moving on to model...\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 1, ignore_mismatched_sizes = True) # np.log(1000)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# if USE_CUDA:\n",
        "#     model = model.cuda()\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     f\"test-{model_name}-finetuned\",\n",
        "#     evaluation_strategy = evaluation_strategy,\n",
        "#     save_strategy = save_strategy,\n",
        "#     learning_rate = learning_rate,\n",
        "#     per_device_train_batch_size=per_device_train_batch_size,\n",
        "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "#     num_train_epochs=num_train_epochs,\n",
        "#     weight_decay=weight_decay,\n",
        "#     load_best_model_at_end=load_best_model_at_end,\n",
        "#     metric_for_best_model=metric_for_best_model,\n",
        "# )\n",
        "# info = [model_name, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, num_train_epochs]\n",
        "\n",
        "# print(\"Instantiating the Trainer...\")\n",
        "# # # Call the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,                         # the instantiated Transformers model to be trained\n",
        "#     args=args,                  # training arguments, defined above\n",
        "#     train_dataset=train_dataset,         # training dataset\n",
        "#     eval_dataset=valid_dataset,          # evaluation dataset\n",
        "\n",
        "#     compute_metrics=compute_metrics_for_regression    # the callback that computes metrics of interest\n",
        "# )\n",
        "\n",
        "# print(\"Training the model...\")\n",
        "# # # # Train the model\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEV9oiW9doVp",
        "outputId": "2d19e565-4690-47a7-978a-3da3e4aa8fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running following model: First try\n",
            "Methodology: First try\n",
            "After making limitations, working with 2856 albums in total...\n",
            "2814/2856 (98.5%) albums, have length >200 and are retained.\n",
            "512\n",
            "False\n",
            "Creating train encodings\n",
            "Creating valid encodings\n"
          ]
        }
      ],
      "source": [
        "methodologies = {0 : ('First try', create_train_test_encodings, MakeTorchData, True, False)}\n",
        "id = uuid.uuid4()\n",
        "methodology = 0\n",
        "\n",
        "methodology_name = methodologies[methodology][0]\n",
        "create_encodings_fx = methodologies[methodology][1]\n",
        "make_datasets_fx = methodologies[methodology][2]\n",
        "standardize_parts = methodologies[methodology][3]\n",
        "see_lbs = methodologies[methodology][4]\n",
        "\n",
        "print(f\"Running following model: {methodology_name}\")\n",
        "print(f\"Methodology: {methodology_name}\")\n",
        "\n",
        "# train_dataset, valid_dataset = create_datasets(tokenizer, create_encodings_fx, make_datasets_fx, standardize_parts, see_lbs)\n",
        "\n",
        "# 1. def create_datasets(tokenizer, create_encodings_fx, make_datasets_fx, standardize_parts = True, see_lbs = False, chunk = False):\n",
        "chunk = True\n",
        "max_length = 512\n",
        "reg_albums = init_albums(path = '', file = 'albums_f.pickle', \n",
        "            standardize_parts = standardize_parts, see_lbs = see_lbs)\n",
        "    \n",
        "x_train, y_train, x_test, y_test = split_datasets(reg_albums, 'c_rate')\n",
        "\n",
        "add_special_tokens = True\n",
        "if chunk:\n",
        "    add_special_tokens = False\n",
        "\n",
        "# 2. def create_train_test_encodings(tokenizer, train_text, test_text, **kwargs):\n",
        "train_encodings, test_encodings = create_encodings_fx(tokenizer, x_train, x_test, \n",
        "                                                      max_length = max_length, \n",
        "                                                      add_special_tokens = add_special_tokens)\n",
        "\n",
        "a = train_encodings['input_ids'][0]\n",
        "b = train_encodings['attention_mask'][0]\n",
        "\n",
        "# print(c[0])\n",
        "# print(train_dataset[0])\n",
        "# print(a)\n",
        "# print(b)\n",
        "# print(type(train_encodings))\n",
        "\n",
        "# print(train_encodings['input_ids'][0:3])\n",
        "# print(train_encodings['input_ids'][0:1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print(\"Creating train encodings\")\n",
        "# train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "# len(train_encodings['input_ids'])\n",
        "# chunksize = max_length - 2\n",
        "\n",
        "# input_id_chunks = list(torch.tensor(train_encodings['input_ids'][0]).split(chunksize - 2))\n",
        "# mask_chunks = list(torch.tensor(train_encodings['attention_mask'][0]).split(chunksize - 2))\n",
        "\n",
        "# for i in range(len(input_id_chunks)):\n",
        "#     # add CLS and SEP tokens to input IDs\n",
        "#     input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "#     # add attention tokens to attention mask\n",
        "#     mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "#     # get required padding length\n",
        "#     pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "#     # check if tensor length satisfies required chunk size\n",
        "#     if pad_len > 0:\n",
        "#         # if padding length is more than 0, we must add padding\n",
        "#         input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "#         mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "\n",
        "# # check length of each tensor\n",
        "# for chunk in input_id_chunks:\n",
        "#     print(len(chunk))\n",
        "# # print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
        "# chunk\n",
        "# [print(i) for i in x_train[:10]]\n",
        "# print(train_encodings)\n",
        "\n",
        "\n",
        "# print(\"Finalized dataset creation, moving on to model...\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 1, ignore_mismatched_sizes = True) # np.log(1000)\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# if USE_CUDA:\n",
        "#     model = model.cuda()\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     f\"test-{model_name}-finetuned\",\n",
        "#     evaluation_strategy = evaluation_strategy,\n",
        "#     save_strategy = save_strategy,\n",
        "#     learning_rate = learning_rate,\n",
        "#     per_device_train_batch_size=per_device_train_batch_size,\n",
        "#     per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "#     num_train_epochs=num_train_epochs,\n",
        "#     weight_decay=weight_decay,\n",
        "#     load_best_model_at_end=load_best_model_at_end,\n",
        "#     metric_for_best_model=metric_for_best_model,\n",
        "# )\n",
        "# info = [model_name, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, num_train_epochs]\n",
        "\n",
        "# print(\"Instantiating the Trainer...\")\n",
        "# # # Call the Trainer\n",
        "# trainer = Trainer(\n",
        "#     model=model,                         # the instantiated Transformers model to be trained\n",
        "#     args=args,                  # training arguments, defined above\n",
        "#     train_dataset=train_dataset,         # training dataset\n",
        "#     eval_dataset=valid_dataset,          # evaluation dataset\n",
        "#     compute_metrics=compute_metrics_for_regression    # the callback that computes metrics of interest\n",
        "# )\n",
        "\n",
        "# print(\"Training the model...\")\n",
        "# # # # Train the model\n",
        "# trainer.train()\n",
        "\n",
        "# print(\"Evaluating the model...\")\n",
        "# # # # Call the summary\n",
        "# # trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImFF4SzmkpcW"
      },
      "outputs": [],
      "source": [
        "def split_datasets(reg_albums, rating_type):\n",
        "    num_train = int(len(reg_albums) * 0.8)\n",
        "    num_test = len(reg_albums) - num_train\n",
        "    train, test = random_split(reg_albums, [num_train, num_test])\n",
        "\n",
        "    x_train = [i[3] for i in train]\n",
        "    x_test = [i[3] for i in test]\n",
        "    \n",
        "    if rating_type == 'c_rate':\n",
        "        y_train = [int(i[1]) for i in train]\n",
        "        y_test = [int(i[1]) for i in test]\n",
        "    elif rating_type == 'u_rate':\n",
        "        y_train = [int(10 * i[2]) for i in train]\n",
        "        y_test = [int(10 * i[2]) for i in test]\n",
        "    \n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "def create_train_test_encodings(tokenizer, train_text, test_text, **kwargs):\n",
        "    max_length = None\n",
        "    if 'max_length' in kwargs:\n",
        "        max_length = kwargs['max_length']\n",
        "    print(max_length)\n",
        "\n",
        "    if 'add_special_tokens' in kwargs:\n",
        "        add_special_tokens = kwargs['add_special_tokens']\n",
        "    print(add_special_tokens)\n",
        "\n",
        "    print(\"Creating train encodings\")\n",
        "    train_encodings = tokenizer(train_text, truncation=True, padding=True, \n",
        "                                max_length=max_length, add_special_tokens = add_special_tokens)\n",
        "    print(\"Creating valid encodings\")\n",
        "    test_encodings = tokenizer(test_text, truncation=True, padding=True,\n",
        "                               max_length=max_length, add_special_tokens = add_special_tokens)\n",
        "    return train_encodings, test_encodings\n",
        "\n",
        "class MakeTorchData(torch.utils.data.Dataset):\n",
        "      def __init__(self, encodings, labels):\n",
        "          self.encodings = encodings\n",
        "          self.labels = labels\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          item = {}\n",
        "          for k, v in self.encodings.items():\n",
        "              if torch.is_tensor(v):\n",
        "                  print(\"yo, it's a tensor\")\n",
        "                  item[k] = v[idx]\n",
        "              else:\n",
        "                item[k] = torch.tensor(v[idx])\n",
        "                print(\"yo, it is not a tensor\")\n",
        "          #     item[k] = v\n",
        "          # item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "          item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "          item[\"labels\"] = float(item[\"labels\"])\n",
        "          return item\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.labels)\n",
        "\n",
        "\n",
        "def chunker(input_ids, attention_mask, curr_label, chunksize):\n",
        "    input_id_chunks = list(torch.tensor(input_ids).split(chunksize - 2))\n",
        "    mask_chunks = list(torch.tensor(attention_mask).split(chunksize - 2))\n",
        "    label_chunks = [curr_label] * len(input_id_chunks)\n",
        "    for i in range(len(input_id_chunks)):\n",
        "      # add CLS and SEP tokens to input IDs\n",
        "      input_id_chunks[i] = torch.cat([torch.tensor([101]), input_id_chunks[i], torch.tensor([102])])\n",
        "      # add attention tokens to attention mask\n",
        "      mask_chunks[i] = torch.cat([torch.tensor([1]), mask_chunks[i], torch.tensor([1])])\n",
        "      # get required padding length\n",
        "      pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "      # check if tensor length satisfies required chunk size\n",
        "      if pad_len > 0:\n",
        "          # if padding length is more than 0, we must add padding\n",
        "          input_id_chunks[i] = torch.cat([input_id_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "          mask_chunks[i] = torch.cat([mask_chunks[i], torch.Tensor([0] * pad_len)])\n",
        "    return input_id_chunks, mask_chunks, label_chunks\n",
        "# chunker(a, b, 512)\n",
        "\n",
        "def chunk_encodings(encodings, labels):\n",
        "    encodings_dict = {'input_ids' : [],\n",
        "                      'attention_mask' : []}\n",
        "    y_train_big = []\n",
        "    for i in range(len(encodings['input_ids'])):\n",
        "        curr_input_ids = encodings['input_ids'][i]\n",
        "        curr_mask = encodings['attention_mask'][i] # train_encodings, test_encodings\n",
        "        curr_label = labels[i] # y_train\n",
        "\n",
        "        input_id_chunks, mask_chunks, label_chunks = chunker(curr_input_ids, curr_mask, curr_label, 512)\n",
        "\n",
        "        encodings_dict['input_ids'].extend(input_id_chunks)\n",
        "        encodings_dict['attention_mask'].extend(mask_chunks)\n",
        "        y_train_big.extend(label_chunks)\n",
        "    return encodings_dict, y_train_big\n",
        "\n",
        "def create_datasets(tokenizer, create_encodings_fx, make_datasets_fx, standardize_parts = True, see_lbs = False, chunk = False):\n",
        "    reg_albums = init_albums(path = '', file = 'albums_f.pickle', \n",
        "            standardize_parts = standardize_parts, see_lbs = see_lbs)\n",
        "    \n",
        "    x_train, y_train, x_test, y_test = split_datasets(reg_albums, 'c_rate')\n",
        "\n",
        "    add_special_tokens = True\n",
        "    if chunk:\n",
        "        add_special_tokens = False\n",
        "    \n",
        "    train_encodings, test_encodings = create_encodings_fx(tokenizer, x_train, x_test, \n",
        "                                                          max_length = max_length, \n",
        "                                                          add_special_tokens = add_special_tokens)\n",
        "\n",
        "    # if chunk:\n",
        "    #     train_encodings_new, y_train_new = chunk_encodings(train_encodings, y_train)\n",
        "    #     test_encodings_new, y_test_new = chunk_encodings(test_encodings, y_test)\n",
        "\n",
        "    train_dataset = make_datasets_fx(train_encodings_new, y_train_new)\n",
        "    valid_dataset = make_datasets_fx(test_encodings_new, y_test_new)\n",
        "    return train_dataset, valid_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEPorUtnGtSB",
        "outputId": "b6af0278-1b0a-4ad3-fab0-ed31d5206136"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1]),\n",
              " 'input_ids': tensor([  101, 30524, 10506, 13106,  6170,  1997,  2115,  9135,  1999,  2023,\n",
              "          5507,  8004,  6313,  9478,  1997, 17128,  8554, 12758,  1999,  2023,\n",
              "          3690,  2302, 13734,  2302, 13734,  1010,  2302, 13734,  2057,  2562,\n",
              "          8119, 15561,  1999,  2023,  3690,  1997, 15862,  2040,  2562, 11065,\n",
              "         24501, 16781,  2013,  1996,  8616,  4355,  1997,  3628,  1010,  3628,\n",
              "          2023, 14099, 27523, 23436,  1045,  1005,  1049,  2469,  2017,  2657,\n",
              "          2009,  2077,  2612,  1997,  8119, 15561,  2191,  2033,  1037, 14412,\n",
              "          7485,  2006,  2115,  2723,  2006,  2115,  2723,  1010,  2006,  2115,\n",
              "          2723,  1012,  1012,  1012, 30525, 30524, 30525, 30524,  2017,  2020,\n",
              "          1037,  9467,  3238,  2775,  2316,  6340,  2011, 10551,  2892, 14731,\n",
              "          2505,  2021, 10256,  2748,  1998,  2053,  2074,  3432,  4694,  1005,\n",
              "          1056,  8826,  2664,  2821,  2748,  1010,  2821,  2053,  2059,  2028,\n",
              "          2154,  2017,  1005,  1040,  2018,  2009, 14146,  2115,  2485, 18934,\n",
              "         28368,  2115,  3899,  1011,  4540,  2098, 20403,  2017,  1005,  2128,\n",
              "          2083,  2007, 14397, 10128, 17629,  2015,  2017,  1005,  2128,  2083,\n",
              "          2007, 14397, 10128, 17629,  2015,  2085,  2017,  1005,  2128,  2083,\n",
              "          2007, 14397, 10128, 17629,  2015,  2182,  2057,  2175, 11094, 15495,\n",
              "          8044,  2005,  4020,  2182,  1005,  1055,  1996,  2518,  2008,  7545,\n",
              "          1996, 19479,  2015,  2000,  1996, 23497,  2182,  1005,  1055,  1996,\n",
              "          2518,  2008,  3084,  2017,  2448,  2005,  1996, 11784,  2182,  2057,\n",
              "          2175, 11094, 15495,  8044,  2005,  4020, 12645,  2059,  2028,  2154,\n",
              "          2017,  1005,  1040,  2018,  2009, 14146,  2115,  2485, 18934, 28368,\n",
              "          2115,  3899,  1011,  4540,  2098, 20403,  2017,  1005,  2128,  2083,\n",
              "          2007, 14397, 10128, 17629,  2015,  2017,  1005,  2128,  2083,  2007,\n",
              "         14397, 10128, 17629,  2015,  2085,  2017,  1005,  2128,  2083,  2007,\n",
              "         14397, 10128, 17629,  2015,  2182,  2057,  2175, 11094, 15495,  8044,\n",
              "          2005,  4020,  2182,  1005,  1055,  1996,  2518,  2008,  7545,  1996,\n",
              "         19479,  2015,  2000,  1996, 23497,  2182,  1005,  1055,  1996,  2518,\n",
              "          2008,  3084,  2017,  2448,  2005,  1996, 11784,  2182,  2057,  2175,\n",
              "         11094, 15495,  8044,  2005,  4020, 12645, 30525, 30524,  2106,  2017,\n",
              "          2507,  2009,  2185,  2106,  2017,  2507,  2009,  2185,  2005,  2489,\n",
              "          2123,  1005,  1056,  2017,  2507,  2009,  2185,  2292,  1005,  1055,\n",
              "          3046,  2000,  2562,  2009,  1999,  1996,  2155,  1045,  2113,  2017,\n",
              "          2113,  4078,  6030,  3508,  2746,  2188,  2007,  2115, 10306,  2440,\n",
              "          1997,  5472,  1045,  2113,  2009,  1005,  1055,  2053, 10885,  2085,\n",
              "          2017,  1005,  2128, 20228, 14138,  2115,  4714,  8983,  1997,  2455,\n",
              "          1999,  1037,  3842,  2104,  2115,  3094,  2106,  2017,  2507,  2009,\n",
              "          2185,  3398,  1045,  2435,  2009,  2185,  2106,  2017,  2507,  2009,\n",
              "          2185,  2005,  2489,  2054,  2052,  2017,  2031,  2149,  3477,  1029,\n",
              "          1045,  2134,  1005,  1056,  2113,  2008,  2115,  2293,  2001,  1037,\n",
              "         19502,  2054,  2055, 12284,  2008,  9041,  2006,  2115,  5995,  1998,\n",
              "          4304,  2054,  2055, 14200,  2115,  6093,  1998, 19287,  2123,  1005,\n",
              "          1056,  2812,  1037,  2518,  2000,  2033,  1999,  2115,  3842,  2007,\n",
              "          2049, 22692,  9598,  2052,  2017,  5342,  1999,  1996, 10974,  2052,\n",
              "          2017,  5342,  1999,  1996, 10974,  2007,  2033,  2180,  1005,  1056,\n",
              "          2017,  5342,  1999,  1996, 10974,  2073,  2009,  1005,  1055,  2601,\n",
              "          1998,  2057,  2064, 20071,  7200,  2030,  2156,  8916,  2004, 21281,\n",
              "         14787,  3508,  2073,  1996, 24623,  1998,  2492, 12328,  2191,  2037,\n",
              "          7939,  2015,  2331,  2011,  2523,  1045, 12860,  1045,  1005,  1040,\n",
              "          2196,  2202,  3087,  2045,  2153,  2000,  2023,  3842,  1037,  3842,\n",
              "          2104,  2115,  3094, 30525, 30524,  2175,  3805,  1998, 26478,   102]),\n",
              " 'labels': 80.0}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKM_QJmplYlN",
        "outputId": "19fab62b-8463-49a8-c398-0f5738978367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([  101, 30524, 10506, 13106,  6170,  1997,  2115,  9135,  1999,  2023,\n",
            "         5507,  8004,  6313,  9478,  1997, 17128,  8554, 12758,  1999,  2023,\n",
            "         3690,  2302, 13734,  2302, 13734,  1010,  2302, 13734,  2057,  2562,\n",
            "         8119, 15561,  1999,  2023,  3690,  1997, 15862,  2040,  2562, 11065,\n",
            "        24501, 16781,  2013,  1996,  8616,  4355,  1997,  3628,  1010,  3628,\n",
            "         2023, 14099, 27523, 23436,  1045,  1005,  1049,  2469,  2017,  2657,\n",
            "         2009,  2077,  2612,  1997,  8119, 15561,  2191,  2033,  1037, 14412,\n",
            "         7485,  2006,  2115,  2723,  2006,  2115,  2723,  1010,  2006,  2115,\n",
            "         2723,  1012,  1012,  1012, 30525, 30524, 30525, 30524,  2017,  2020,\n",
            "         1037,  9467,  3238,  2775,  2316,  6340,  2011, 10551,  2892, 14731,\n",
            "         2505,  2021, 10256,  2748,  1998,  2053,  2074,  3432,  4694,  1005,\n",
            "         1056,  8826,  2664,  2821,  2748,  1010,  2821,  2053,  2059,  2028,\n",
            "         2154,  2017,  1005,  1040,  2018,  2009, 14146,  2115,  2485, 18934,\n",
            "        28368,  2115,  3899,  1011,  4540,  2098, 20403,  2017,  1005,  2128,\n",
            "         2083,  2007, 14397, 10128, 17629,  2015,  2017,  1005,  2128,  2083,\n",
            "         2007, 14397, 10128, 17629,  2015,  2085,  2017,  1005,  2128,  2083,\n",
            "         2007, 14397, 10128, 17629,  2015,  2182,  2057,  2175, 11094, 15495,\n",
            "         8044,  2005,  4020,  2182,  1005,  1055,  1996,  2518,  2008,  7545,\n",
            "         1996, 19479,  2015,  2000,  1996, 23497,  2182,  1005,  1055,  1996,\n",
            "         2518,  2008,  3084,  2017,  2448,  2005,  1996, 11784,  2182,  2057,\n",
            "         2175, 11094, 15495,  8044,  2005,  4020, 12645,  2059,  2028,  2154,\n",
            "         2017,  1005,  1040,  2018,  2009, 14146,  2115,  2485, 18934, 28368,\n",
            "         2115,  3899,  1011,  4540,  2098, 20403,  2017,  1005,  2128,  2083,\n",
            "         2007, 14397, 10128, 17629,  2015,  2017,  1005,  2128,  2083,  2007,\n",
            "        14397, 10128, 17629,  2015,  2085,  2017,  1005,  2128,  2083,  2007,\n",
            "        14397, 10128, 17629,  2015,  2182,  2057,  2175, 11094, 15495,  8044,\n",
            "         2005,  4020,  2182,  1005,  1055,  1996,  2518,  2008,  7545,  1996,\n",
            "        19479,  2015,  2000,  1996, 23497,  2182,  1005,  1055,  1996,  2518,\n",
            "         2008,  3084,  2017,  2448,  2005,  1996, 11784,  2182,  2057,  2175,\n",
            "        11094, 15495,  8044,  2005,  4020, 12645, 30525, 30524,  2106,  2017,\n",
            "         2507,  2009,  2185,  2106,  2017,  2507,  2009,  2185,  2005,  2489,\n",
            "         2123,  1005,  1056,  2017,  2507,  2009,  2185,  2292,  1005,  1055,\n",
            "         3046,  2000,  2562,  2009,  1999,  1996,  2155,  1045,  2113,  2017,\n",
            "         2113,  4078,  6030,  3508,  2746,  2188,  2007,  2115, 10306,  2440,\n",
            "         1997,  5472,  1045,  2113,  2009,  1005,  1055,  2053, 10885,  2085,\n",
            "         2017,  1005,  2128, 20228, 14138,  2115,  4714,  8983,  1997,  2455,\n",
            "         1999,  1037,  3842,  2104,  2115,  3094,  2106,  2017,  2507,  2009,\n",
            "         2185,  3398,  1045,  2435,  2009,  2185,  2106,  2017,  2507,  2009,\n",
            "         2185,  2005,  2489,  2054,  2052,  2017,  2031,  2149,  3477,  1029,\n",
            "         1045,  2134,  1005,  1056,  2113,  2008,  2115,  2293,  2001,  1037,\n",
            "        19502,  2054,  2055, 12284,  2008,  9041,  2006,  2115,  5995,  1998,\n",
            "         4304,  2054,  2055, 14200,  2115,  6093,  1998, 19287,  2123,  1005,\n",
            "         1056,  2812,  1037,  2518,  2000,  2033,  1999,  2115,  3842,  2007,\n",
            "         2049, 22692,  9598,  2052,  2017,  5342,  1999,  1996, 10974,  2052,\n",
            "         2017,  5342,  1999,  1996, 10974,  2007,  2033,  2180,  1005,  1056,\n",
            "         2017,  5342,  1999,  1996, 10974,  2073,  2009,  1005,  1055,  2601,\n",
            "         1998,  2057,  2064, 20071,  7200,  2030,  2156,  8916,  2004, 21281,\n",
            "        14787,  3508,  2073,  1996, 24623,  1998,  2492, 12328,  2191,  2037,\n",
            "         7939,  2015,  2331,  2011,  2523,  1045, 12860,  1045,  1005,  1040,\n",
            "         2196,  2202,  3087,  2045,  2153,  2000,  2023,  3842,  1037,  3842,\n",
            "         2104,  2115,  3094, 30525, 30524,  2175,  3805,  1998, 26478,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1]), 'labels': 80.0}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wul_Pkw4-__I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0308fab4dcb448228e9b41ac53f64cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a704bc8e70c4780866ffeba8a325d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18acc9c0e6194cf2b5bb1010ae12ae02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35272d617bcf45dfac00420278eea79b",
            "placeholder": "",
            "style": "IPY_MODEL_8a3755a65c004fec8b1b89340ece7533",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "19c1e6c6552845999c3a9cfde3e3d972": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25cbc225f9e74d73921bde58a48510cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e480f9a5e94914af20f420e02e6f79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "281db7bc45fc4d4e8e0cfa4f2acf6b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29e928265c8f4c2fa983b4870d731910": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25cbc225f9e74d73921bde58a48510cc",
            "placeholder": "",
            "style": "IPY_MODEL_19c1e6c6552845999c3a9cfde3e3d972",
            "value": " 629/629 [00:00&lt;00:00, 6.23kB/s]"
          }
        },
        "2f7b3fc1d1d74520a9437bc41461dde5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35272d617bcf45dfac00420278eea79b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d028338e0d4235a912c6564dac0235": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a8dda7dd2c747c18858fcf3129561c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c32f27667d824138b0f309f7596ee46c",
              "IPY_MODEL_b54616e138e5498c9a627863a87bd05e",
              "IPY_MODEL_29e928265c8f4c2fa983b4870d731910"
            ],
            "layout": "IPY_MODEL_d874fc6ba49f46b39fb92818ee5e5e82"
          }
        },
        "4ad49e41bf744450b81f3da81b01ee90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c3ce84bad264a8bae525e22d11772c6",
            "placeholder": "",
            "style": "IPY_MODEL_281db7bc45fc4d4e8e0cfa4f2acf6b05",
            "value": " 226k/226k [00:00&lt;00:00, 554kB/s]"
          }
        },
        "51d38bf4df404b2f86bb33db43fa6bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26e480f9a5e94914af20f420e02e6f79",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65e1d8e4d0c74ab1ae9f6b409bd7989f",
            "value": 48
          }
        },
        "5675bd10fe874e239c974677b5cef207": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65e1d8e4d0c74ab1ae9f6b409bd7989f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "693c9e7b5b7f419bb6f23fad55a87e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a3755a65c004fec8b1b89340ece7533": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9816859db7434245950a92c72e9f7e56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3ce84bad264a8bae525e22d11772c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ece9fc77a5941f4b0e0f444fbf753d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9816859db7434245950a92c72e9f7e56",
            "placeholder": "",
            "style": "IPY_MODEL_f5d997cc7dd54d91adb12cf14939643b",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "9feaf92695eb4eb793d6262958c9bdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ece9fc77a5941f4b0e0f444fbf753d0",
              "IPY_MODEL_e2105eefe33a4646b8774da10082c45c",
              "IPY_MODEL_4ad49e41bf744450b81f3da81b01ee90"
            ],
            "layout": "IPY_MODEL_44d028338e0d4235a912c6564dac0235"
          }
        },
        "b37851f63a68496e8826f326294f858b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0308fab4dcb448228e9b41ac53f64cb6",
            "placeholder": "",
            "style": "IPY_MODEL_bca8269cdc4c433aa419a683a7d960d1",
            "value": " 48.0/48.0 [00:00&lt;00:00, 964B/s]"
          }
        },
        "b54616e138e5498c9a627863a87bd05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5675bd10fe874e239c974677b5cef207",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b58160dfaab9450da3b1a8c1f147f8b1",
            "value": 629
          }
        },
        "b58160dfaab9450da3b1a8c1f147f8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bca8269cdc4c433aa419a683a7d960d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c32f27667d824138b0f309f7596ee46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f7b3fc1d1d74520a9437bc41461dde5",
            "placeholder": "",
            "style": "IPY_MODEL_693c9e7b5b7f419bb6f23fad55a87e7b",
            "value": "Downloading config.json: 100%"
          }
        },
        "d3d23b23216148d785929cd0e324bd1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18acc9c0e6194cf2b5bb1010ae12ae02",
              "IPY_MODEL_51d38bf4df404b2f86bb33db43fa6bf7",
              "IPY_MODEL_b37851f63a68496e8826f326294f858b"
            ],
            "layout": "IPY_MODEL_e9514eb50be94889a042e50b460c02e4"
          }
        },
        "d79fb131a3064523b87759729d6af229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d874fc6ba49f46b39fb92818ee5e5e82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2105eefe33a4646b8774da10082c45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d79fb131a3064523b87759729d6af229",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a704bc8e70c4780866ffeba8a325d25",
            "value": 231508
          }
        },
        "e9514eb50be94889a042e50b460c02e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5d997cc7dd54d91adb12cf14939643b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
