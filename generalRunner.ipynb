{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Note: this is .ipynb equivalent of generalRunner.py\n","\"\"\""]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":876,"status":"ok","timestamp":1660185150736,"user":{"displayName":"Kevin Niu","userId":"01236558301534034885"},"user_tz":240},"id":"IzIUIyt2z9sJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/kniu91/opt/anaconda3/envs/nlp_new2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import sys\n","import os\n","import torch\n","import csv\n","import argparse\n","from functools import partial\n","import itertools\n","import uuid\n","import pickle\n","import numpy as np\n","        \n","from torchtext.data.utils import get_tokenizer\n","tokenizer = get_tokenizer('basic_english')\n","from torchtext.vocab import Vocab, build_vocab_from_iterator\n","from torchtext.vocab import GloVe\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data.dataset import random_split\n","from torch.nn.utils.rnn import pad_sequence\n","import time\n","import importlib\n","from nltk.corpus import stopwords\n","from torch.utils.data.dataset import random_split\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":158,"status":"ok","timestamp":1660184977066,"user":{"displayName":"Kevin Niu","userId":"01236558301534034885"},"user_tz":240},"id":"S2S_Yy240O9X"},"outputs":[],"source":["# Need if running on Google Colab\n","# !pip install selenium\n","# !pip install webdriver-manager\n","# !pip install pickle5\n","# import nltk\n","# nltk.download('stopwords')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2242,"status":"ok","timestamp":1660185154728,"user":{"displayName":"Kevin Niu","userId":"01236558301534034885"},"user_tz":240},"id":"Kn9EiUynz9sN","outputId":"95864a0c-ea81-4c6f-cdce-6cb13939abf8"},"outputs":[],"source":["COLAB = False\n","\n","USE_CUDA = False\n","if COLAB:\n","    from google.colab import drive \n","    drive.mount('/content/gdrive')\n","    PATH = 'gdrive/MyDrive/nlp22/project/'\n","    sys.path.append('gdrive/MyDrive/nlp22/project')\n","\n","    USE_CUDA = torch.cuda.is_available()\n","\n","    if USE_CUDA:\n","        DEVICE = torch.device('cuda')\n","        print(\"Using cuda.\")\n","    else:\n","        DEVICE = torch.device('cpu')\n","        print(\"Using cpu.\")\n","\n","    os.chdir(os.path.join(os.getcwd(),'gdrive/MyDrive/nlp22/project'))\n","    \n","from album_loader import *\n","import lyric_loader\n","import nlpmodel\n","importlib.reload(nlpmodel)\n","\n","VECTORS_CACHE_DIR = './.vector_cache'\n","\n","UNK, PAD, LBS, LBE, SBS, SBE, PART = 0, 1, 2, 3, 4, 5, 6\n","FIRST_TOKENS = 5000\n","STRATEGY = f'FIRST {FIRST_TOKENS} - Embeddings On'\n","EMBEDDING_DIMENSIONS = 300\n","\n","VECTORS_CACHE_DIR = './.vector_cache' # or modify to the correct location\n","\n","TOKEN_CHANGES = {'motherfuckin' : 'fucking', 'drippin' : 'dripping', \n","'prayin' : 'praying', 'countin' : 'counting', 'knowin' : 'knowing', \n","'listenin' : 'listening', 'showin' : 'showing', 'whippin' : 'whipping', \n","'spendin' : 'spending', 'stuntin' : 'stunting', 'starin' : 'staring', \n","'trappin' : 'trapping', 'wonderin' : 'wondering', 'mothafuckin' : 'fucking', \n","'motherfucking' : 'fucking','winnin' : 'winning', 'grindin' : 'grinding', \n","'pourin' : 'pouring', 'breathin' : 'breathing', 'lettin' : 'letting', \n","'switchin' : 'switching', 'flexin' : 'flexing', 'speakin' : 'speaking',\n","'the—' : 'the', 'thе' : 'the', 'aight' : 'alright', 'a-' : 'a',\n","'hunnid' : 'hundred', 'prolly' : 'probably'}\n","\n","RATE_TYPE = 'c_rate'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":157,"status":"ok","timestamp":1660185155749,"user":{"displayName":"Kevin Niu","userId":"01236558301534034885"},"user_tz":240},"id":"LpzoRdNZz9sO"},"outputs":[],"source":["def init_albums(path, file, standardize_parts, see_lbs, u_rate_min = 15):\n","    \"\"\"\n","    Instantiates a set of Albums for regression purposes\n","\n","    kwargs:\n","    file  -- file containing Albums info (albums_f.pickle)\n","    standardize_parts - signals whether to standardize parts in lyrics\n","    see_lbs - signals whether to see linen breaks in lyrics\n","    u_rate_min - minimium user ratings to be included in data set\n","    \"\"\"\n","    print(\"Init albums was called...\")\n","    albums_data = os.path.join(path, file)\n","    albums_pre = lyric_loader.RegAlbums(album_path = albums_data, \n","                                        standardize_parts = standardize_parts, \n","                                        see_line_breaks = see_lbs,\n","                                        u_rate_min = u_rate_min)\n","    reg_albums = albums_pre.reg_full_album_text() \n","    return reg_albums\n","\n","def weed_albums_fx(reg_albums, exc_sds):\n","    \"\"\"\n","    Excludes a portion of medium-rated albums\n","\n","    kwargs:\n","    reg_albums -- regression albums\n","    exc_sds -- how many standard deviations to exclude\n","    \"\"\"\n","    a = np.array([int(i[1]) for i in reg_albums])\n","    \n","    denom = exc_sds * 2\n","    lo = a.mean() - (np.std(a) / denom)\n","    hi = a.mean() + (np.std(a) / denom) \n","    test = [i for i in reg_albums if int(i[1]) < lo or int(i[1]) > hi]\n","    print(f\"Albums reduced from {len(reg_albums)} to {len(test)}\")\n","    return test\n","\n","def yield_tokens(data_iter, chars):\n","    if chars:\n","        for _, _, _, text in data_iter:\n","            for i in text:\n","                if i == ' ':\n","                    continue\n","                yield i\n","    else:\n","        for _, _, _, text in data_iter:\n","            res = [TOKEN_CHANGES.get(i, i) for i in tokenizer(text)]\n","            yield res\n","\n","def set_vocab(reg_albums, min_freq, chars, treat_stops):\n","    specials = ['<unk>', '<pad>', '<lb>', '</lb>', '<sb>', '</sb>', '[part]', '[sw]']\n","    yield_fx = yield_tokens\n","    if chars:\n","        specials = ['<unk>', '<pad>']\n","    vocab = build_vocab_from_iterator(yield_fx(reg_albums, chars = chars),\n","                specials = specials, min_freq = min_freq)\n","    vocab.set_default_index(vocab['<unk>'])\n","    return vocab\n","\n","def create_train_test(standardize_parts, see_lbs):\n","    \"\"\"\n","    Create separate train and test datasets\n","        according to pre-processing required\n","    \n","    kwargs:\n","    standardize_parts -- signals whether to standardize parts in lyrics\n","    see_lbs -- signals whether to see linen breaks in lyrics    \n","    \"\"\"\n","    print(\"Create train test was called...\")\n","    sp = 1 if standardize_parts else 0\n","    sl = 1 if see_lbs else 0\n","    u_rate_min = 10\n","    reg_albums = init_albums(path = '', file = 'albums_f.pickle', \n","                standardize_parts = standardize_parts, see_lbs = see_lbs, u_rate_min = u_rate_min)\n","\n","    fourth = len(reg_albums) // 4\n","    train_val, test = random_split(reg_albums, [len(reg_albums) - fourth, fourth])\n","\n","    comb = (train_val, test)\n","    with open(f'train_val_test_{sp}_{sl}.pickle', 'wb') as handle:\n","        pickle.dump(comb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    return comb\n","\n","def load_train_test(standardize_parts, see_lbs):\n","    \"\"\"\n","    Load train and test datasets\n","    according to pre-processing required. Will\n","    create if not already created.\n","    \n","    kwargs:\n","    standardize_parts -- signals whether to standardize parts in lyrics\n","    see_lbs -- signals whether to see linen breaks in lyrics    \n","    \"\"\"\n","    \n","    sp = 1 if standardize_parts else 0\n","    sl = 1 if see_lbs else 0\n","    try:\n","        with open(f'train_val_test_{sp}_{sl}.pickle', 'rb') as handle:\n","            comb = pickle.load(handle)   \n","    except:\n","        print(f\"Creating train/val/test sets for standardize_parts: {standardize_parts}, see_lbs: {see_lbs}\")\n","        comb = create_train_test(standardize_parts, see_lbs)\n","    train, test = comb\n","    return train, test"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def create_datasets(reg_albums, test_reg_albums, final):\n","    \"\"\"\n","    Create train, val and test data sets for THIS run of the model\n","\n","    kwargs:\n","    reg_albums -- training data from load_train_test()\n","    test_reg_albums -- test data from load_train_test()\n","    final - dictates how train,test,val will be created\n","        - if True, test_reg_albums (\"actual\" test set) will be \n","            used as test set\n","        - if False, assume hyperparameter tuning, so a portion\n","            of the train_val dataset will be cut out to\n","            evaluate parameter\n","    \"\"\"\n","    print(\"Creating datasets right now...\")\n","    prop = 0.8\n","\n","    if final:\n","        print(\"Running on 'final'\")\n","        print(f\"Final train set is {len(reg_albums)}\")\n","        num_train = int(len(reg_albums) * .9)    \n","        num_valid = len(reg_albums) - num_train\n","        train_data, valid_data = random_split (reg_albums, [num_train, num_valid])\n","        print(f\"Train is size: {len(train_data)}, valid is size: {len(valid_data)}, test is size: {len(test_reg_albums)}\")\n","        return train_data, valid_data, test_reg_albums\n","\n","    num_train_valid = int(len(reg_albums) * prop)\n","    num_test = len(reg_albums) - num_train_valid\n","    train_valid_data, test_data = random_split(reg_albums, [num_train_valid, num_test])\n","\n","    num_train = int(num_train_valid * .9)\n","    num_valid = num_train_valid - num_train\n","    train_data, valid_data = random_split(train_valid_data, [num_train, num_valid])\n","\n","    print(f\"Train samples: {num_train}\\nValid samples: {num_valid}\\nTest samples: {num_test}\")\n","    return train_data, valid_data, test_data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660185157074,"user":{"displayName":"Kevin Niu","userId":"01236558301534034885"},"user_tz":240},"id":"oIu3EW13z9sP"},"outputs":[],"source":["# Methodlogies made up of nn.Modules and their collate functions\n","def main(methodologies,\n","        methodology = 0,\n","        weed_albums = False,\n","        standardize_parts = True, \n","        see_lbs = True,\n","        treat_stops = 'see',\n","        opt = 'adam', \n","        lr = 0.005, \n","        wd = 0,  \n","        chunk_size = 2500,\n","        chunk_portion = 'first',\n","        epochs = 10,\n","        batch_size = 16,\n","        test_batch_size = 16,\n","        clip = 1,\n","        rnn_type = 'LSTM',\n","        embedding_size = 300,\n","        use_glove = True, \n","        bidirectional = False, \n","        num_layers = 1,\n","        dropout = 0,\n","        use_cuda = USE_CUDA,\n","        rate_type = 'c_rate',\n","        valid = False,\n","        final = False,\n","        kfoldername = ''\n","       ):\n","    \"\"\"\n","    Runs model from end to end, from importing regression albums\n","    to training, evalluation, and saving results\n","    \"\"\"\n","\n","    id = uuid.uuid4()\n","    char = False\n","    if methodology == 7:\n","        char = True\n","    if methodology == 8:\n","        chunk_size = 'all'\n","\n","    methodology_name = methodologies[methodology][0]\n","    hidden_size = int(embedding_size * 2)\n","    if embedding_size > 300:\n","        use_glove = False\n","    print(f\"Running following model:\")\n","    print(f\"Methodology: {methodology_name}\\nWeed albums:{weed_albums}\")\n","    print(f\"\\nOptimizer: {opt}\\nLearning Rate: {lr}\")\n","    print(f\"Epochs: {epochs}\\nEmbedding size: {embedding_size}\\nHidden size: {hidden_size}\")\n","    print(f\"Batch size: {batch_size},\\nTest batch size: {test_batch_size}\")\n","    print(f\"Bidirectional: {bidirectional},\\nNumber of layers: {num_layers}\")\n","    print(f\"Character-level RNN included: {char}\")\n","    print(f\"Standardize_parts: {standardize_parts}\\nSee line breaks: {see_lbs}\\nTreat stops: {treat_stops}\")\n","    print(f\"Chunking portion used : {chunk_portion}\")\n","    print(f\"This is treated as validation verison? {valid}\")\n","    print(f\"This is treated as final verison? {final}\")\n","    \n","    reg_albums, test_reg_albums = load_train_test(standardize_parts, see_lbs)\n","    print(f\"Working with {len(reg_albums)} albums in total for reg_albums...\")\n","    if final:\n","        print(f\"Working with {len(test_reg_albums)} albums in total for test_reg_albums...\")\n","    \n","    wd_albs = 0\n","    if weed_albums:\n","        reg_albums = weed_albums_fx(reg_albums, 1)\n","        wd_albs = 1\n","\n","    train_dataset, valid_dataset, test_dataset = create_datasets(reg_albums, \n","                                                                    test_reg_albums,\n","                                                                    final)\n","    \n","    train_text = train_dataset\n","    vocab = None\n","    glove_vectors = None\n","    char_vocab = None\n","    vocab = set_vocab(train_text, min_freq = 10, chars = False, treat_stops = treat_stops)\n","    if methodology > 0:\n","        glove = GloVe('6B',cache=VECTORS_CACHE_DIR)\n","        glove_vectors = glove.get_vecs_by_tokens(vocab.get_itos())\n","    if methodology == 7:\n","        char_vocab = set_vocab(reg_albums, min_freq = 1000, chars = True)\n","    \n","    print(\"Finished loading vocabs and GLoVE\")\n","\n","    vocab_size = len(vocab) if vocab is not None else 0 \n","    char_vocab_size = len(char_vocab) if char_vocab is not None else 0\n","    rnn_kwargs =  {'rnn_type': rnn_type, \n","                    'vocab_size': vocab_size,\n","                    'char_vocab_size' : char_vocab_size,\n","                    'embedding_dim': embedding_size, \n","                    'hidden_dim': hidden_size,\n","                    'num_layers': num_layers,\n","                    'bidirectional' : bidirectional,\n","                    'char' : char,\n","                    'use_glove' : use_glove,\n","                    'glove_vectors' : glove_vectors,\n","                    'freeze_glove' : False,\n","                    'dropout': 0}\n","\n","    strategy = methodologies[methodology][0]\n","    info_pre = [id, methodology, strategy, standardize_parts, see_lbs, treat_stops, opt, lr, wd, chunk_size, chunk_portion,\n","                         epochs, batch_size, test_batch_size, hidden_size, use_glove, bidirectional, num_layers, \n","                         rate_type, char, weed_albums]\n","\n","    model_template = nlpmodel.NlpModel(*methodologies[methodology][1:], \n","                                        rnn_kwargs, \n","                                        info_pre,\n","                                        use_cuda = USE_CUDA)\n","    model = model_template.model\n","\n","    print(\"Initialized model\")\n","\n","    optim_dict = {'adam' : torch.optim.Adam, 'sgd' : torch.optim.SGD}\n","    try:\n","        optimizer = optim_dict[opt.lower()](model.parameters(), lr = lr, weight_decay = wd)\n","    except:\n","        print(\"Please select a valid optimizer! Specify either 'adam' or 'sgd'.\")\n","\n","    print(\"Initialized optimizer\")\n","\n","    collate_kwargs = {'vocab' : vocab, \n","                    'rate_type' : rate_type,\n","                    'char_vocab' : char_vocab, \n","                    'chunk_size' : chunk_size,\n","                    'chunk_portion' : chunk_portion,\n","                    'treat_stops' : treat_stops}\n","\n","    train_dataloader, valid_dataloader, test_dataloader = model_template.collate_datasets(train_dataset,\n","                                                                                    valid_dataset,\n","                                                                                    test_dataset,\n","                                                                                    batch_size,\n","                                                                                    test_batch_size,\n","                                                                                    valid,   \n","                                                                                    **collate_kwargs)\n","    print(\"Initialized dataloaders. Running models now...\")\n","    last_mse, last_var, best_model, best_r2, best_epoch = model_template.runModel(optimizer, train_dataloader, valid_dataloader, clip, epochs)\n","    \n","    link = 'model_results_valid.csv'\n","        \n","    test_mse_new, test_var_new, my_r2, skl_r2  = None, None, None, None\n","\n","    if final:    \n","        link = 'model_results_final.csv'\n","        torch.save(best_model, f'bestModel - {methodology}.pt')\n","        print(\"Here are results on final dataset...\")\n","    test_mse_new, test_var_new, my_r2, skl_r2  = model_template.get_accuracy(test_dataloader, best_model, \n","                                                                                    info_pre, model_template.use_cuda)\n","\n","    with open(link, 'a') as csvfile:\n","        print(f\"Writing to {link}\")\n","        writer = csv.writer(csvfile)\n","        writer.writerow(info_pre + [last_mse, last_var, test_mse_new, test_var_new, my_r2, skl_r2, best_r2, best_epoch])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaklTWqIz9sQ","outputId":"14f6eda3-663f-4ef6-fc7f-018a2f9cc02e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running following model:\n","Methodology: CBOW - unfrozen\n","Weed albums:False\n","\n","Optimizer: adam\n","Learning Rate: 0.0005\n","Epochs: 25\n","Embedding size: 300\n","Hidden size: 600\n","Batch size: 16,\n","Test batch size: 16\n","Bidirectional: False,\n","Number of layers: 1\n","Character-level RNN included: False\n","Standardize_parts: False\n","See line breaks: True\n","Treat stops: remove\n","Chunking portion used : first\n","This is treated as validation verison? True\n","This is treated as final verison? False\n","Working with 2622 albums in total for reg_albums...\n","Creating datasets right now...\n","Train samples: 1887\n","Valid samples: 210\n","Test samples: 525\n","Finished loading vocabs and GLoVE\n","\n","Model called is a CBOW\n","CBOW hidden size is 600\n","Initialized model\n","Initialized optimizer\n","Creating training dataset...\n","Creating validation dataset...\n","Creating test dataset...\n","Initialized dataloaders. Running models now...\n","7042.5\n","0: torch.Size([22730, 16])\n","tensor(5501.4683, grad_fn=<MseLossBackward0>)\n","\n","3735.0\n","5828.8125\n","5436.375\n","6119.5625\n","6254.625\n","5939.8125\n","7267.1875\n","5799.5625\n","7385.125\n","3664.25\n","3407.25\n","5352.0\n","5210.6875\n","4288.25\n","4999.6875\n","4570.625\n","4345.0\n","6218.5\n","5229.3125\n","5858.5\n","4513.9375\n","4696.5625\n","5000.75\n","5207.3125\n","5485.625\n","5983.3125\n","5547.1875\n","6391.75\n","4316.1875\n","4635.625\n","3956.5\n","5605.6875\n","32: torch.Size([18382, 16])\n","tensor(5173.0693, grad_fn=<MseLossBackward0>)\n","\n","4635.0\n","5903.1875\n","5811.4375\n","3127.0625\n","7267.5625\n","4622.0625\n","7236.375\n","4807.8125\n","5335.5625\n","4061.625\n","6468.25\n","4934.375\n","4484.75\n","6360.25\n","5867.625\n","5501.0\n","5416.625\n","3151.125\n","5173.375\n","3945.6875\n","5138.1875\n","3736.75\n","6332.6875\n","6141.6875\n","4629.6875\n","4568.375\n","6161.25\n","4802.625\n","6865.5\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/kniu91/Documents/Kevin's Folders/Grad Schools/UChicago MPCS/Summer 2022/mpcs53113/finalProject/generalRunner1.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 113>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m main_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mmethodology\u001b[39m\u001b[39m'\u001b[39m : methodology,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mweed_albums\u001b[39m\u001b[39m'\u001b[39m : weed_albums,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mstandardize_parts\u001b[39m\u001b[39m'\u001b[39m : standardize_parts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m'\u001b[39m : final,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mkfoldername\u001b[39m\u001b[39m'\u001b[39m : kfoldername}\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39m# UNK, PAD, LBS, LBE, SBS, SBE, PART, SW = 0, 1, 2, 3, 4, 5, 6, 7\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m main(methodologies, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmain_kwargs)\n","\u001b[1;32m/Users/kniu91/Documents/Kevin's Folders/Grad Schools/UChicago MPCS/Summer 2022/mpcs53113/finalProject/generalRunner1.ipynb Cell 7\u001b[0m in \u001b[0;36mmain\u001b[0;34m(methodologies, methodology, weed_albums, standardize_parts, see_lbs, treat_stops, opt, lr, wd, chunk_size, chunk_portion, epochs, batch_size, test_batch_size, clip, rnn_type, embedding_size, use_glove, bidirectional, num_layers, dropout, use_cuda, rate_type, valid, final, kfoldername)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m train_dataloader, valid_dataloader, test_dataloader \u001b[39m=\u001b[39m model_template\u001b[39m.\u001b[39mcollate_datasets(train_dataset,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m                                                                                 valid_dataset,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m                                                                                 test_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m                                                                                 valid,   \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m                                                                                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcollate_kwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitialized dataloaders. Running models now...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m last_mse, last_var, best_model, best_r2, best_epoch \u001b[39m=\u001b[39m model_template\u001b[39m.\u001b[39;49mrunModel(optimizer, train_dataloader, valid_dataloader, clip, epochs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m link \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmodel_results_valid.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/kniu91/Documents/Kevin%27s%20Folders/Grad%20Schools/UChicago%20MPCS/Summer%202022/mpcs53113/finalProject/generalRunner1.ipynb#W5sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m test_mse_new, test_var_new, my_r2, skl_r2  \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/Documents/Kevin's Folders/Grad Schools/UChicago MPCS/Summer 2022/mpcs53113/finalProject/nlpmodel.py:498\u001b[0m, in \u001b[0;36mNlpModel.runModel\u001b[0;34m(self, optimizer, train_dataloader, valid_dataloader, clip, epochs)\u001b[0m\n\u001b[1;32m    496\u001b[0m best_epoch \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    497\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_fx(train_dataloader, model, optimizer, clip, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_cuda)\n\u001b[1;32m    499\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m+\u001b[39m [epoch]\n\u001b[1;32m    500\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResults after epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/Kevin's Folders/Grad Schools/UChicago MPCS/Summer 2022/mpcs53113/finalProject/nlpmodel.py:390\u001b[0m, in \u001b[0;36mtrain_an_epoch\u001b[0;34m(dataloader, model, optimizer, clip, use_cuda)\u001b[0m\n\u001b[1;32m    388\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    389\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(), clip)\n\u001b[0;32m--> 390\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n","File \u001b[0;32m~/opt/anaconda3/envs/nlp_new2/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/nlp_new2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/opt/anaconda3/envs/nlp_new2/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    142\u001b[0m            grads,\n\u001b[1;32m    143\u001b[0m            exp_avgs,\n\u001b[1;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m            state_steps,\n\u001b[1;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n","File \u001b[0;32m~/opt/anaconda3/envs/nlp_new2/lib/python3.10/site-packages/torch/optim/_functional.py:105\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    103\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    109\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[1;32m    110\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\"\"\"\n","Define inputs to main function here\n","\n","- Methodology must be integer from 0-8,\n","  indicating which model to apply\n","- 0, 1, 2, 3, and 8 are in paper\n","\"\"\"\n","\n","\n","methodologies= {0 : ('BOW', nlpmodel.collate_into_bow, \n","                                nlpmodel.collate_into_bow, \n","                                nlpmodel.train_an_epoch, \n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.BoWClassifier),\n","                1 : ('CBOW - unfrozen', \n","                                nlpmodel.collate_into_cbow_unfrozen, \n","                                nlpmodel.collate_into_cbow_unfrozen,\n","                                nlpmodel.train_an_epoch, \n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.CBoWClassifier),\n","                2 : ('RNN - album firstk',  \n","                                nlpmodel.collate_batch_rnn_firstk, \n","                                nlpmodel.collate_batch_rnn_firstk,\n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.RNNClassifier),\n","                3 : ('RNN - song firstk',  \n","                                nlpmodel.collate_batch_rnn_firstk_song, \n","                                nlpmodel.collate_batch_rnn_firstk_song, \n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.RNNClassifier),\n","                4: ('RNN - album firstk - test whole',  \n","                                nlpmodel.collate_batch_rnn_firstk, \n","                                nlpmodel.collate_rnn_whole,\n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.RNNClassifier),\n","                5 : ('RNN - song firstk - test whole',  \n","                                nlpmodel.collate_batch_rnn_firstk_song, \n","                                nlpmodel.collate_rnn_whole, \n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.RNNClassifier),\n","                6 : ('RNN - chunk - test whole',\n","                                nlpmodel.collate_batch_chunk,\n","                                nlpmodel.collate_rnn_whole,\n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy,\n","                                nlpmodel.RNNClassifier),             \n","                7 : ('RNN - character',\n","                                nlpmodel.collate_batch_chars,\n","                                nlpmodel.collate_batch_chars,\n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy,\n","                                nlpmodel.RNNClassifier),\n","                8 : ('RNN - album all',  \n","                                nlpmodel.collate_batch_rnn_firstk, \n","                                nlpmodel.collate_batch_rnn_firstk,\n","                                nlpmodel.train_an_epoch,\n","                                nlpmodel.get_accuracy, \n","                                nlpmodel.RNNClassifier)\n","                }\n","\n","methodology = 1\n","weed_albums = False\n","standardize_parts = False\n","see_lbs = True\n","treat_stops = 'remove' # 'see', 'remove', None\n","\n","opt = 'adam'\n","lr = 0.0005\n","wd = 0\n","chunk_size = 5000\n","chunk_portion = 'first' # 'first', 'mid', 'last'\n","batch_size = 16\n","test_batch_size = 16\n","epochs = 25\n","clip = 1\n","\n","rnn_type = 'LSTM' # 'LSTM'\n","embedding_size = 300\n","use_glove = True\n","bidirectional = False\n","num_layers = 1\n","dropout = 0\n","\n","rate_type = RATE_TYPE\n","\n","valid = True\n","final = False\n","kfoldername = 'method0-testsp.pickle'\n","\n","main_kwargs = {'methodology' : methodology,\n","                'weed_albums' : weed_albums,\n","                'standardize_parts' : standardize_parts,\n","                'see_lbs' : see_lbs,\n","                'treat_stops' : treat_stops,\n","                'opt' : opt,\n","                'lr' : lr,\n","                'wd' : wd,\n","                'chunk_size' : chunk_size,\n","                'batch_size' : batch_size,\n","                'test_batch_size' : test_batch_size,\n","                'epochs' : epochs,\n","                'clip' : 1,\n","                'rnn_type': rnn_type, \n","                'embedding_size' : embedding_size,\n","                'use_glove' : use_glove,\n","                'bidirectional' : bidirectional,\n","                'num_layers' : num_layers,\n","                'dropout' : dropout,\n","                'use_cuda' : USE_CUDA,\n","                'rate_type' : rate_type,\n","                'valid' : valid,\n","                'final' : final,\n","                'kfoldername' : kfoldername}\n","\n","# UNK, PAD, LBS, LBE, SBS, SBE, PART, SW = 0, 1, 2, 3, 4, 5, 6, 7\n","main(methodologies, **main_kwargs)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"generalRunner.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 ('nlp_new2')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"1e08048f17ff008031266911b66a2ac7278627f8d3490a4b4484f3664274d5bf"}}},"nbformat":4,"nbformat_minor":0}
